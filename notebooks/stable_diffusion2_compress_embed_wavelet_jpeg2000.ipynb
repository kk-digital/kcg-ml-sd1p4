{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6VYov4wgzms"
      },
      "source": [
        "# Compressing an embedding using wavelets (jpeg2000) algorithm\n",
        "## [jpeg-2000-wavelet-compression](http://www.jeanfeydy.com/Teaching/MasterClass_Radiologie/Part%207%20-%20JPEG2000%20compression.html)\n",
        "\n",
        "This notebook demonstrates generating an embedding, saving it, loading it, and using the jpeg2000 algorithm in order to compress it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1iumXeh69Pa"
      },
      "outputs": [],
      "source": [
        "ENV_TYPE = \"TEST1\"\n",
        "\n",
        "if(ENV_TYPE != \"TEST\"):\n",
        "  !git clone \"https://github.com/kk-digital/kcg-ml-sd1p4.git\"\n",
        "  %cd kcg-ml-sd1p4\n",
        "  !pip3 install -r requirements.txt\n",
        "  exit()\n",
        "  base_directory = \"./\"\n",
        "else:\n",
        "  base_directory = \"../\"\n",
        "\n",
        "# Magical check for fixing all of our directory issues\n",
        "import subprocess\n",
        "output = subprocess.check_output([\"pwd\"], universal_newlines=True)\n",
        "if \"notebooks\" in output:\n",
        "    %cd ..\n",
        "del output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-15sYL8Vgzmw"
      },
      "outputs": [],
      "source": [
        "# Check for dependency needed for using OpenCV\n",
        "import subprocess\n",
        "\n",
        "result = subprocess.run(['dpkg', '-s', 'libgl1-mesa-glx'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "# If the package is not installed, install it\n",
        "if 'is not installed and no information is available' in result.stderr:\n",
        "    print(\"Installing libgl, which is needed to run the GA script.\")\n",
        "    subprocess.run([\"apt\", \"update\"])\n",
        "    subprocess.run([\"apt\", \"install\", \"libgl1-mesa-glx\"])\n",
        "else:\n",
        "    print(\"Package 'libgl1-mesa-glx' is already installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbV9eAw169Pc"
      },
      "outputs": [],
      "source": [
        "!python3 ./download_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVGtkQBh69Pc"
      },
      "outputs": [],
      "source": [
        "!python3 ./process_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6917Ijqv69Pd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import time\n",
        "import shutil\n",
        "from torchvision.transforms import ToPILImage\n",
        "from os.path import join\n",
        "\n",
        "base_directory = \"./\"\n",
        "sys.path.insert(0, base_directory)\n",
        "\n",
        "from stable_diffusion.model_paths import *\n",
        "from configs.model_config import ModelPathConfig\n",
        "from stable_diffusion.utils_backend import *\n",
        "from stable_diffusion.utils_image import *\n",
        "from stable_diffusion.utils_model import *\n",
        "from stable_diffusion.stable_diffusion import StableDiffusion\n",
        "from utility.labml import monit\n",
        "\n",
        "\n",
        "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
        "output_directory = join(output_base_dir, \"jpeg_embed_compression/\")\n",
        "\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(output_directory)\n",
        "except Exception as e:\n",
        "    print(e, \"\\n\", \"Creating the path...\")\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "else:\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "def to_pil(image):\n",
        "    return ToPILImage()(torch.clamp((image + 1.0) / 2.0, min=0.0, max=1.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k44DDtTq69Pe"
      },
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "base_dir = os.getcwd()\n",
        "sys.path.insert(0, base_dir)\n",
        "\n",
        "batch_size = 1\n",
        "model_config = ModelPathConfig()\n",
        "pt = IODirectoryTree(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvURFyxE69Pe"
      },
      "outputs": [],
      "source": [
        "# initialize an empty stable diffusion class\n",
        "stable_diffusion = StableDiffusion(device=device)\n",
        "get_memory_status(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tgS-j5U69Pf"
      },
      "outputs": [],
      "source": [
        "# initialize an empty latent diffusion model; it returns self.model\n",
        "# then load the clip text embedder from the path `pt.embedder_path` with .load_clip_embedder()\n",
        "# it returns the clip embedder, so you can chain a .load_submodels() to load the text embedder submodels\n",
        "\n",
        "stable_diffusion.quick_initialize().load_clip_embedder().load_submodels()\n",
        "get_memory_status(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJLRPgfZ69Pg"
      },
      "outputs": [],
      "source": [
        "stable_diffusion.model.clip_embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xKekGteH69Ph"
      },
      "outputs": [],
      "source": [
        "# get the embedding for a prompt\n",
        "prompt_embedding = stable_diffusion.model.clip_embedder(\n",
        "    [\"Just another prompt embedding\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IolCk_X269Pi"
      },
      "outputs": [],
      "source": [
        "get_memory_status(device)\n",
        "prompt_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua1Lzdu769Pi"
      },
      "outputs": [],
      "source": [
        "# Unload clip since we no longer need it\n",
        "stable_diffusion.model.unload_clip_embedder()\n",
        "get_memory_status(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l7Y_OYAx69Pj"
      },
      "outputs": [],
      "source": [
        "# Save the prompt embedding\n",
        "torch.save(prompt_embedding, join(output_directory, \"prompt_embedding_uncompressed.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQu3RTZngzm4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Load the prompt embedding\n",
        "prompt_embedding = torch.load(join(output_directory, \"prompt_embedding_uncompressed.pt\"))\n",
        "prompt_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0p00Ilj2gzm5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Assuming 'prompt_embedding' is your tensor\n",
        "\n",
        "# Convert the PyTorch tensor to a numpy array\n",
        "prompt_embedding_np = prompt_embedding.cpu().detach().numpy()\n",
        "\n",
        "# Normalize the data to the range [0, 255]\n",
        "prompt_embedding_np = (prompt_embedding_np - prompt_embedding_np.min()) / (prompt_embedding_np.max() - prompt_embedding_np.min()) * 255\n",
        "\n",
        "# Convert to uint8 data type\n",
        "prompt_embedding_np = prompt_embedding_np.astype(np.uint8)\n",
        "\n",
        "# Convert to Pillow image\n",
        "prompt_embedding_img = Image.fromarray(prompt_embedding_np.squeeze())\n",
        "\n",
        "# Save the compressed image as a .jpg file with quality set to 95\n",
        "prompt_embedding_img.save(join(output_directory, \"prompt_embedding_compressed.jp2\"), format=\"JPEG2000\", quality=\"100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MZ_HbXTfgzm5"
      },
      "outputs": [],
      "source": [
        "# Load the compressed .jpg image\n",
        "loaded_image = Image.open(join(output_directory, \"prompt_embedding_compressed.jp2\"))\n",
        "\n",
        "# Convert the loaded image to a numpy array\n",
        "loaded_embedding_np = np.array(loaded_image)\n",
        "\n",
        "# Convert the numpy array back to a PyTorch tensor and move to the same device\n",
        "loaded_embedding_tensor = torch.from_numpy(loaded_embedding_np).unsqueeze(0).float().to(get_device())\n",
        "\n",
        "# Move loaded tensor to GPU\n",
        "loaded_embedding_tensor = loaded_embedding_tensor.to(get_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R250I2zgzm6"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the loaded tensor\n",
        "loaded_embedding_tensor.shape  # Should be (77, 768)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Stable Diffusion\n",
        "stable_diffusion.quick_initialize().load_autoencoder().load_decoder()\n",
        "stable_diffusion.model.load_unet()"
      ],
      "metadata": {
        "id": "RtIMYp-NkC7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
        "clip_text_embedder.load_submodels()\n",
        "\n",
        "null_cond = clip_text_embedder(\"\")\n",
        "clip_text_embedder.unload_submodels()"
      ],
      "metadata": {
        "id": "cQHLS0zBryQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate image to test tensor\n",
        "images = stable_diffusion.generate_images_latent_from_embeddings(null_prompt=null_cond, embedded_prompt=prompt_embedding)\n",
        "get_memory_status(get_device())\n",
        "img = to_pil(images[0].squeeze())\n",
        "img.show()"
      ],
      "metadata": {
        "id": "LESMC1SYkgxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}