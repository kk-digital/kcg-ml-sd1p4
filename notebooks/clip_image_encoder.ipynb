{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOGHpzunNTP8"
   },
   "outputs": [],
   "source": [
    "from configs.model_config import ModelPathConfig\n",
    "!git clone https: // github.com/kk-digital/kcg-ml-sd1p4\n",
    "%cd kcg-ml-sd1p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIQeYagINTP9"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYUd5ggfNTP-"
   },
   "outputs": [],
   "source": [
    "!python3./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axjphIXzNTP-"
   },
   "outputs": [],
   "source": [
    "!python3./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GdGs7yX-NTP_"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from os.path import join\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
    "output_directory = join(output_base_dir, \"clip_image_encoder/\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_directory)\n",
    "except Exception as e:\n",
    "    print(e, \"\\n\", \"Creating the path...\")\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "from stable_diffusion.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
    "from stable_diffusion.utils_model import *\n",
    "from stable_diffusion.utils_backend import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_f2hX3_NTP_"
   },
   "outputs": [],
   "source": [
    "# Setup config (paths and such)\n",
    "batch_size = 1\n",
    "base_dir = os.getcwd()\n",
    "config = ModelPathConfig(root_directory=base_dir)\n",
    "config.create_directory_tree_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApPvWyIcNTQA"
   },
   "outputs": [],
   "source": [
    "# Initialize clip\n",
    "clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
    "clip_text_embedder.load_submodels(tokenizer_path=config.get_model(\"clip/txt_emb_tokenizer\"),\n",
    "                                  transformer_path=config.get_model(\"clip/txt_emb_transformer\"))\n",
    "get_memory_status(device=get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bwbv2wiCNTQB"
   },
   "outputs": [],
   "source": [
    "# get the text embeddings\n",
    "null_prompt = torch.tensor(()).to(get_device())\n",
    "embeddings = clip_text_embedder.forward(\"A computer virus dancing tango.\")\n",
    "get_memory_status(device=get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0ZTYQeqNTQC"
   },
   "outputs": [],
   "source": [
    "# check their shape\n",
    "null_prompt.shape, embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cleanup clip\n",
    "del clip_text_embedder"
   ],
   "metadata": {
    "id": "IogztH-hXpYe"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "y18p_s0vNTQC"
   },
   "outputs": [],
   "source": [
    "# save them to disk\n",
    "torch.save(embeddings, join(output_directory, \"embeddings.pt\"))\n",
    "torch.save(null_prompt, join(output_directory, \"null_prompt.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNZLJ15cNTQD"
   },
   "outputs": [],
   "source": [
    "# Load Stable Diffusion\n",
    "sd = StableDiffusion(device=get_device(), n_steps=25)\n",
    "sd.quick_initialize().load_autoencoder(config.get_model(\"vae/vae\")).load_decoder(config.get_model(\"vae/vae_decoder\"))\n",
    "sd.model.load_unet(config.get_model(\"unet/unet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iap5siT7NTQD"
   },
   "outputs": [],
   "source": [
    "images = sd.generate_images_from_embeddings(null_prompt=null_prompt, embedded_prompt=embeddings, batch_size=batch_size)\n",
    "get_memory_status(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKgLb_iyNTQE"
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5Mq8c-wNTQE"
   },
   "outputs": [],
   "source": [
    "from stable_diffusion.utils_image import to_pil\n",
    "\n",
    "if batch_size > 1:\n",
    "    grid = torchvision.utils.make_grid(images, nrow=2, normalize=False, range=(-1, 1))\n",
    "    img = to_pil(grid)\n",
    "else:\n",
    "    img = to_pil(images[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYEE7uiENTQE"
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoaFWspMNTQI"
   },
   "outputs": [],
   "source": [
    "img_encoder = CLIPImageEncoder(device=get_device())\n",
    "get_memory_status(device=get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRdwsiehNTQJ"
   },
   "outputs": [],
   "source": [
    "img_encoder.load_submodels()\n",
    "get_memory_status(device=get_device())\n",
    "img_encoder.initialize_preprocessor(do_center_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNOSoKppNTQJ"
   },
   "outputs": [],
   "source": [
    "img_encoder.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PyTOJUONTQJ"
   },
   "outputs": [],
   "source": [
    "prep_from_img = img_encoder.preprocess_input(img).to(get_device())\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd-BtK_BNTQK"
   },
   "outputs": [],
   "source": [
    "img_encoder.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6jXNVO_NTQK"
   },
   "outputs": [],
   "source": [
    "prep_from_tensor = img_encoder.preprocess_input(images)\n",
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_kiwfVRNTQK"
   },
   "outputs": [],
   "source": [
    "prep_from_img.shape, prep_from_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6-sv-21NTQK"
   },
   "outputs": [],
   "source": [
    "torch.all(prep_from_img.to(get_device()) == prep_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zH-kZ7jNTQL"
   },
   "outputs": [],
   "source": [
    "torch.norm(prep_from_img.to(get_device()) - prep_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PKc_ZKZNTQL"
   },
   "outputs": [],
   "source": [
    "to_pil(prep_from_img.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4CynljoNTQL"
   },
   "outputs": [],
   "source": [
    "to_pil(prep_from_tensor.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVB9yOV0NTQL"
   },
   "outputs": [],
   "source": [
    "to_pil((prep_from_img - prep_from_tensor).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtfIf3bLNTQL"
   },
   "outputs": [],
   "source": [
    "if batch_size > 1:\n",
    "    grid = torchvision.utils.make_grid([prep_from_img, prep_from_tensor], nrow=2, normalize=False, range=(-1, 1))\n",
    "    img = to_pil(grid)\n",
    "else:\n",
    "    img = to_pil(prep_from_img.squeeze(0))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "3OpFxAEVNTQL"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "grid = torchvision.utils.make_grid([prep_from_img.squeeze(), prep_from_tensor.squeeze()], nrow=2, normalize=False,\n",
    "                                   range=(-1, 1))\n",
    "img = to_pil(grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
