{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "import shutil\n",
    "from os.path import join\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
    "output_directory = join(output_base_dir, \"clip_image_encoder/\")\n",
    "\n",
    "try: \n",
    "    shutil.rmtree(output_directory)\n",
    "except Exception as e:\n",
    "    print(e, \"\\n\", \"Creating the path...\")\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "from stable_diffusion.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
    "from utility.labml.monit import section\n",
    "# from stable_diffusion.utils.utils import SectionManager as section\n",
    "from stable_diffusion.utils_model import *\n",
    "from stable_diffusion.utils_backend import *\n",
    "from stable_diffusion.constants import IODirectoryTree\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "device = get_device(None)\n",
    "pt = IODirectoryTree(base_directory=base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = StableDiffusion(device=device)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start embedding a prompt and saving it\n",
    "\n",
    "sd.quick_initialize().load_clip_embedder(**pt.embedder).load_submodels(**pt.embedder_submodels)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text embeddings\n",
    "uncond, cond = sd.get_text_conditioning(uncond_scale = 7.5, prompts = [\"A computer virus dancing tango.\"])\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check their shape\n",
    "uncond.shape, cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save them to disk\n",
    "torch.save(uncond, join(output_directory, \"uncond.pt\"))\n",
    "torch.save(uncond, join(output_directory, \"cond.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the unet, needed for the sampling\n",
    "sd.model.load_unet(**pt.unet)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the decoder, needed for getting the sampled image back from the latent space\n",
    "sd.model.load_autoencoder(**pt.autoencoder).load_decoder(**pt.decoder)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sd.generate_images_from_embeddings(null_prompt = uncond, embedded_prompt=cond, batch_size = batch_size)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_size > 1:\n",
    "    grid = torchvision.utils.make_grid(images, nrow=2, normalize=False, range=(-1, 1))\n",
    "    img = to_pil(grid)\n",
    "else:\n",
    "    img = to_pil(images[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.unload_model()\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder = CLIPImageEncoder(device=device)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder.load_clip_model(**pt.clip_model)\n",
    "get_memory_status()\n",
    "img_encoder.initialize_preprocessor(do_center_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_from_img = img_encoder.preprocess_input(img).to(device)\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_from_tensor = img_encoder.preprocess_input(images)\n",
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_from_img.shape, prep_from_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(prep_from_img.to(device) == prep_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(prep_from_img.to(device) - prep_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil(prep_from_img.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil(prep_from_tensor.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil((prep_from_img - prep_from_tensor).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_size > 1:\n",
    "    grid = torchvision.utils.make_grid([prep_from_img, prep_from_tensor], nrow=2, normalize=False, range=(-1, 1))\n",
    "    img = to_pil(grid)\n",
    "else:\n",
    "    img = to_pil(prep_from_img.squeeze(0))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torchvision.utils.make_grid([prep_from_img.squeeze(), prep_from_tensor.squeeze()], nrow=2, normalize=False, range=(-1, 1))\n",
    "img = to_pil(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features_tensor = img_encoder(prep_from_tensor)\n",
    "img_features_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features_image = img_encoder(prep_from_img)\n",
    "img_features_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(img_features_image - img_features_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
