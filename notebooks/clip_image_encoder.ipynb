{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOGHpzunNTP8"
   },
   "outputs": [],
   "source": [
    "ENV_TYPE = \"TEST\"\n",
    "\n",
    "if(ENV_TYPE != \"TEST\"):\n",
    "  !git clone \"https://github.com/kk-digital/kcg-ml-sd1p4.git\"\n",
    "  %cd kcg-ml-sd1p4\n",
    "  !pip3 install -r requirements.txt\n",
    "  exit()\n",
    "  base_directory = \"./\"\n",
    "else:\n",
    "  base_directory = \"../\"\n",
    "\n",
    "# Magical check for fixing all of our directory issues\n",
    "import subprocess\n",
    "output = subprocess.check_output([\"pwd\"], universal_newlines=True)\n",
    "if \"notebooks\" in output:\n",
    "    %cd ..\n",
    "del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYUd5ggfNTP-"
   },
   "outputs": [],
   "source": [
    "!python3 ./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axjphIXzNTP-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 ./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdGs7yX-NTP_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "import shutil\n",
    "from os.path import join\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
    "output_directory = join(output_base_dir, \"clip_image_encoder/\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_directory)\n",
    "except Exception as e:\n",
    "    print(e, \"\\n\", \"Creating the path...\")\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "from stable_diffusion.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from utility.labml.monit import section\n",
    "# from stable_diffusion.utils.utils import SectionManager as section\n",
    "from stable_diffusion.utils_model import *\n",
    "from stable_diffusion.utils_backend import *\n",
    "from stable_diffusion.model_paths import IODirectoryTree\n",
    "from configs.model_config import ModelPathConfig\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_f2hX3_NTP_"
   },
   "outputs": [],
   "source": [
    "# Setup config (paths and such)\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "sys.path.insert(0, base_dir)\n",
    "\n",
    "batch_size = 1\n",
    "model_config = ModelPathConfig()\n",
    "pt = IODirectoryTree(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApPvWyIcNTQA"
   },
   "outputs": [],
   "source": [
    "# Initialize clip\n",
    "clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
    "clip_text_embedder.load_submodels()\n",
    "get_memory_status(device=get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwbv2wiCNTQB"
   },
   "outputs": [],
   "source": [
    "# get the text embeddings\n",
    "null_prompt = torch.tensor(()).to(get_device())\n",
    "embeddings = clip_text_embedder.forward(\"A computer virus dancing tango.\")\n",
    "get_memory_status(device=get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0ZTYQeqNTQC"
   },
   "outputs": [],
   "source": [
    "# check their shape\n",
    "null_prompt.shape, embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IogztH-hXpYe"
   },
   "outputs": [],
   "source": [
    "# Cleanup clip\n",
    "del clip_text_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y18p_s0vNTQC"
   },
   "outputs": [],
   "source": [
    "# save them to disk\n",
    "torch.save(embeddings, join(output_directory, \"embeddings.pt\"))\n",
    "torch.save(null_prompt, join(output_directory, \"null_prompt.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNZLJ15cNTQD"
   },
   "outputs": [],
   "source": [
    "# Load Stable Diffusion\n",
    "sd = StableDiffusion(device=get_device(), n_steps=25)\n",
    "sd.quick_initialize().load_autoencoder().load_decoder()\n",
    "sd.model.load_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iap5siT7NTQD"
   },
   "outputs": [],
   "source": [
    "images = sd.generate_images_latent_from_embeddings(null_prompt = null_prompt, embedded_prompt=embeddings, batch_size = batch_size)\n",
    "get_memory_status(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKgLb_iyNTQE"
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5Mq8c-wNTQE"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from stable_diffusion.utils_image import to_pil\n",
    "if batch_size > 1:\n",
    "    grid = torchvision.utils.make_grid(images, nrow=2, normalize=False, range=(-1, 1))\n",
    "    img = to_pil(grid)\n",
    "else:\n",
    "    img = to_pil(images[0].squeeze())\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYEE7uiENTQE"
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoaFWspMNTQI"
   },
   "outputs": [],
   "source": [
    "img_encoder = CLIPImageEncoder(device=get_device())\n",
    "get_memory_status(device = get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRdwsiehNTQJ"
   },
   "outputs": [],
   "source": [
    "img_encoder.load_submodels()\n",
    "get_memory_status(device = get_device())\n",
    "img_encoder.initialize_preprocessor(do_center_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNOSoKppNTQJ"
   },
   "outputs": [],
   "source": [
    "img_encoder.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PyTOJUONTQJ"
   },
   "outputs": [],
   "source": [
    "prep_from_img = img_encoder.preprocess_input(img)\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd-BtK_BNTQK"
   },
   "outputs": [],
   "source": [
    "img_encoder.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6jXNVO_NTQK"
   },
   "outputs": [],
   "source": [
    "prep_from_tensor = img_encoder.preprocess_input(images)\n",
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_kiwfVRNTQK"
   },
   "outputs": [],
   "source": [
    "prep_from_img.shape, prep_from_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6-sv-21NTQK"
   },
   "outputs": [],
   "source": [
    "torch.all(prep_from_img.to(get_device()) == prep_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zH-kZ7jNTQL"
   },
   "outputs": [],
   "source": [
    "torch.norm(prep_from_img.to(get_device()) - prep_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PKc_ZKZNTQL"
   },
   "outputs": [],
   "source": [
    "to_pil(prep_from_img.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4CynljoNTQL"
   },
   "outputs": [],
   "source": [
    "to_pil(prep_from_tensor.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVB9yOV0NTQL"
   },
   "outputs": [],
   "source": [
    "to_pil((prep_from_img - prep_from_tensor).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtfIf3bLNTQL"
   },
   "outputs": [],
   "source": [
    "if batch_size > 1:\n",
    "    grid = torchvision.utils.make_grid([prep_from_img, prep_from_tensor], nrow=2, normalize=False, range=(-1, 1))\n",
    "    img = to_pil(grid)\n",
    "else:\n",
    "    img = to_pil(prep_from_img.squeeze(0))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OpFxAEVNTQL"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "grid = torchvision.utils.make_grid([prep_from_img.squeeze(), prep_from_tensor.squeeze()], nrow=2, normalize=False, range=(-1, 1))\n",
    "img = to_pil(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
