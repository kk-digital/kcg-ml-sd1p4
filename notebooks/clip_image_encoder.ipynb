{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GOGHpzunNTP8",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:50:36.331346Z",
     "end_time": "2023-08-17T18:50:36.357633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4\n"
     ]
    }
   ],
   "source": [
    "ENV_TYPE = \"TEST\"\n",
    "\n",
    "if(ENV_TYPE != \"TEST\"):\n",
    "  !git clone \"https://github.com/kk-digital/kcg-ml-sd1p4.git\"\n",
    "  %cd kcg-ml-sd1p4\n",
    "  !pip3 install -r requirements.txt\n",
    "  exit()\n",
    "  base_directory = \"./\"\n",
    "else:\n",
    "  base_directory = \"../\"\n",
    "\n",
    "# Magical check for fixing all of our directory issues\n",
    "import subprocess\n",
    "output = subprocess.check_output([\"pwd\"], universal_newlines=True)\n",
    "if \"notebooks\" in output:\n",
    "    %cd ..\n",
    "del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aYUd5ggfNTP-",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:50:36.358902Z",
     "end_time": "2023-08-17T18:50:41.855259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\u001B[4mCreating directory tree folders.\u001B[0m                                                        \r\n",
      "\t\u001B[1;32mINFO: All model paths have been created.\u001B[0m\r\n",
      "Creating directory tree folders.\u001B[32m...[DONE]\u001B[0m\u001B[34m\t0.74ms\u001B[0m                                  \r\n",
      "\u001B[1;32mINFO: Downloading models. This may take a while.\u001B[0m\r\n",
      "Checking if minio server is accessible...\r\n",
      "Minio server is not accessible...\r\n",
      "Downloading CLIP model\u001B[32m...[DONE]\u001B[0m\u001B[34m\t1.05ms\u001B[0m                                            \r\n",
      "Downloading Base Diffusion model\u001B[32m...[DONE]\u001B[0m\u001B[34m\t0.13ms\u001B[0m                                  \r\n"
     ]
    }
   ],
   "source": [
    "!python3 ./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axjphIXzNTP-",
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-08-17T18:37:31.699923Z",
     "end_time": "2023-08-17T18:38:09.608199Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./input/model/clip/vit-large-patch14/vit-large-patch14.safetensors were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'logit_scale', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias']\r\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "\u001B[1m\u001B[4mInitialize CLIP image encoder and load submodels from lib\u001B[0m                               \r\n",
      "\t\u001B[1;33mWARNING: image_processor has not been given. Initialize one with the `.initialize_preprocessor()` method.\u001B[0m\r\n",
      "Initialize CLIP image encoder and load submodels from lib\u001B[32m...[DONE]\u001B[0m\u001B[34m\t2,292.72ms\u001B[0m     \r\n",
      "\u001B[1m\u001B[4mCLIP: Saving image encoder submodels\u001B[0m                                                    \r\n",
      "\timage_processor saved to:  ./input/model/clip/img_enc_processor\r\n",
      "\tvision_model saved to:  ./input/model/clip/img_enc_vision\r\n",
      "\tCLIP ImageEncoder saved to: ./input/model/clip/img_enc/img_enc.safetensors\r\n",
      "CLIP: Saving image encoder submodels\u001B[32m...[DONE]\u001B[0m\u001B[34m\t1,294.75ms\u001B[0m                          \r\n",
      "\u001B[1m\u001B[4mInitializing Stable Diffusion Model\u001B[0m                                                     \r\n",
      "    encoder initialization\u001B[32m...[DONE]\u001B[0m\u001B[34m\t177.17ms\u001B[0m                                      \r\n",
      "    decoder initialization\u001B[32m...[DONE]\u001B[0m\u001B[34m\t251.10ms\u001B[0m                                      \r\n",
      "  Autoencoder initialization\u001B[32m...[DONE]\u001B[0m\u001B[34m\t432.21ms\u001B[0m                                    \r\n",
      "  U-Net initialization\u001B[32m...[DONE]\u001B[0m\u001B[34m\t6,159.35ms\u001B[0m                                        \r\n",
      "  Latent Diffusion model initialization\u001B[32m...[DONE]\u001B[0m\u001B[34m\t10.85ms\u001B[0m                          \r\n",
      "  stable diffusion checkpoint loading, from ./input/model/sd/v1-5-pruned-emaonly/v1-5-pruned-emaonly.safetensors\u001B[32m...[DONE]\u001B[0m\u001B[34m\t274.18ms\u001B[0m\r\n",
      "\t\tmissing keys 2: ['beta', 'alpha_bar']\r\n",
      "\t\textra keys 14: ['alphas_cumprod', 'alphas_cumprod_prev', 'betas', 'log_one_minus_alphas_cumprod', 'model_ema.decay', 'model_ema.num_updates', 'posterior_log_variance_clipped', 'posterior_mean_coef1', 'posterior_mean_coef2', 'posterior_variance', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod', 'sqrt_recip_alphas_cumprod', 'sqrt_recipm1_alphas_cumprod']\r\n",
      "  model state loading\u001B[32m...[DONE]\u001B[0m\u001B[34m\t10,115.62ms\u001B[0m                                        \r\n",
      "\t===============================================================================================\r\n",
      "\tLayer (type:depth-idx)                                                 Param #\r\n",
      "\t===============================================================================================\r\n",
      "\tLatentDiffusion                                                        2,000\r\n",
      "\t├─UNetWrapper: 1-1                                                     --\r\n",
      "\t│    └─UNetModel: 2-1                                                  --\r\n",
      "\t│    │    └─Sequential: 3-1                                            2,050,560\r\n",
      "\t│    │    └─ModuleList: 3-2                                            249,624,320\r\n",
      "\t│    │    └─TimestepEmbedSequential: 3-3                               97,038,080\r\n",
      "\t│    │    └─ModuleList: 3-4                                            510,795,840\r\n",
      "\t│    │    └─Sequential: 3-5                                            12,164\r\n",
      "\t├─Autoencoder: 1-2                                                     --\r\n",
      "\t│    └─Encoder: 2-2                                                    --\r\n",
      "\t│    │    └─Conv2d: 3-6                                                3,584\r\n",
      "\t│    │    └─ModuleList: 3-7                                            23,627,136\r\n",
      "\t│    │    └─Module: 3-8                                                10,494,976\r\n",
      "\t│    │    └─GroupNorm: 3-9                                             1,024\r\n",
      "\t│    │    └─Conv2d: 3-10                                               36,872\r\n",
      "\t│    └─Decoder: 2-3                                                    --\r\n",
      "\t│    │    └─Conv2d: 3-11                                               18,944\r\n",
      "\t│    │    └─Module: 3-12                                               10,494,976\r\n",
      "\t│    │    └─ModuleList: 3-13                                           38,972,544\r\n",
      "\t│    │    └─GroupNorm: 3-14                                            256\r\n",
      "\t│    │    └─Conv2d: 3-15                                               3,459\r\n",
      "\t│    └─Conv2d: 2-4                                                     72\r\n",
      "\t│    └─Conv2d: 2-5                                                     20\r\n",
      "\t├─CLIPTextEmbedder: 1-3                                                --\r\n",
      "\t│    └─CLIPTextModel: 2-6                                              --\r\n",
      "\t│    │    └─CLIPTextTransformer: 3-16                                  123,060,480\r\n",
      "\t===============================================================================================\r\n",
      "\tTotal params: 1,066,237,307\r\n",
      "\tTrainable params: 1,066,235,307\r\n",
      "\tNon-trainable params: 2,000\r\n",
      "\t===============================================================================================\r\n",
      "Initializing Stable Diffusion Model\u001B[32m...[DONE]\u001B[0m\u001B[34m\t19,469.81ms\u001B[0m                          \r\n",
      "\u001B[1m\u001B[4mStable Diffusion: saving vae submodels\u001B[0m                                                  \r\n",
      "\tSaved decoder to ./input/model/vae/decoder/decoder.safetensors\r\n",
      "Stable Diffusion: saving vae submodels\u001B[32m...[DONE]\u001B[0m\u001B[34m\t624.83ms\u001B[0m                          \r\n",
      "Stable Diffusion: unloading vae submodels\u001B[32m...[DONE]\u001B[0m\u001B[34m\t106.13ms\u001B[0m                       \r\n",
      "\u001B[1m\u001B[4mStable Diffusion: saving embedder submodels\u001B[0m                                             \r\n",
      "\ttransformer saved to:  ./input/model/clip/txt_emb_model\r\n",
      "Stable Diffusion: saving embedder submodels\u001B[32m...[DONE]\u001B[0m\u001B[34m\t595.19ms\u001B[0m                     \r\n",
      "Stable Diffusion: unloading embedder submodels\u001B[32m...[DONE]\u001B[0m\u001B[34m\t179.73ms\u001B[0m                  \r\n"
     ]
    }
   ],
   "source": [
    "!python3 ./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdGs7yX-NTP_",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:09.671377Z",
     "end_time": "2023-08-17T18:38:12.002432Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "import shutil\n",
    "from os.path import join\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
    "output_directory = join(output_base_dir, \"clip_image_encoder/\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_directory)\n",
    "except Exception as e:\n",
    "    print(e, \"\\n\", \"Creating the path...\")\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "from stable_diffusion.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from utility.labml.monit import section\n",
    "# from stable_diffusion.utils.utils import SectionManager as section\n",
    "from stable_diffusion.utils_model import *\n",
    "from stable_diffusion.utils_backend import *\n",
    "from stable_diffusion.model_paths import IODirectoryTree\n",
    "from configs.model_config import ModelPathConfig\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_f2hX3_NTP_",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:12.004225Z",
     "end_time": "2023-08-17T18:38:12.006860Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Setup config (paths and such)\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "sys.path.insert(0, base_dir)\n",
    "\n",
    "batch_size = 1\n",
    "model_config = ModelPathConfig()\n",
    "pt = IODirectoryTree(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApPvWyIcNTQA",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:12.081853Z",
     "end_time": "2023-08-17T18:38:13.265592Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize clip\n",
    "clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
    "clip_text_embedder.load_submodels()\n",
    "get_memory_status(device=get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwbv2wiCNTQB",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:13.266404Z",
     "end_time": "2023-08-17T18:38:13.628013Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# get the text embeddings\n",
    "null_prompt = torch.tensor(()).to(get_device())\n",
    "embeddings = clip_text_embedder.forward(\"A computer virus dancing tango.\")\n",
    "get_memory_status(device=get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0ZTYQeqNTQC",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:13.626700Z",
     "end_time": "2023-08-17T18:38:13.633378Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check their shape\n",
    "null_prompt.shape, embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IogztH-hXpYe",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:13.636790Z",
     "end_time": "2023-08-17T18:38:13.638267Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup clip\n",
    "del clip_text_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y18p_s0vNTQC",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:13.642653Z",
     "end_time": "2023-08-17T18:38:13.643989Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# save them to disk\n",
    "torch.save(embeddings, join(output_directory, \"embeddings.pt\"))\n",
    "torch.save(null_prompt, join(output_directory, \"null_prompt.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNZLJ15cNTQD",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:13.645269Z",
     "end_time": "2023-08-17T18:38:25.290090Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load Stable Diffusion\n",
    "sd = StableDiffusion(device=get_device(), n_steps=25)\n",
    "sd.quick_initialize().load_autoencoder().load_decoder()\n",
    "sd.model.load_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iap5siT7NTQD",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:25.286647Z",
     "end_time": "2023-08-17T18:38:52.115168Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "images = sd.generate_images_latent_from_embeddings(null_prompt = null_prompt, embedded_prompt=embeddings, batch_size = batch_size)\n",
    "get_memory_status(get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKgLb_iyNTQE",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:44:30.813800Z",
     "end_time": "2023-08-17T18:44:30.817694Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5Mq8c-wNTQE",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:38:52.125585Z",
     "end_time": "2023-08-17T18:38:52.219089Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from stable_diffusion.utils_image import to_pil\n",
    "if batch_size > 1:\n",
    "    grid = torchvision.utils.make_grid(images, nrow=2, normalize=False, range=(-1, 1))\n",
    "    img = to_pil(grid)\n",
    "else:\n",
    "    img = to_pil(images[0].squeeze())\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYEE7uiENTQE",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:47:08.875088Z",
     "end_time": "2023-08-17T18:47:08.893291Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoaFWspMNTQI",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:47:08.880852Z",
     "end_time": "2023-08-17T18:47:08.910028Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "img_encoder = CLIPImageEncoder(device=get_device())\n",
    "get_memory_status(device = get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRdwsiehNTQJ",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:47:08.888431Z",
     "end_time": "2023-08-17T18:47:15.593611Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "img_encoder.load_submodels()\n",
    "get_memory_status(device = get_device())\n",
    "img_encoder.initialize_preprocessor(do_center_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNOSoKppNTQJ",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:47:15.595297Z",
     "end_time": "2023-08-17T18:47:15.598540Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "img_encoder.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PyTOJUONTQJ",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:47:15.601161Z",
     "end_time": "2023-08-17T18:47:15.660060Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "prep_from_img = img_encoder.preprocess_input(img)\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd-BtK_BNTQK",
    "ExecuteTime": {
     "start_time": "2023-08-17T18:47:15.616377Z",
     "end_time": "2023-08-17T18:47:15.660362Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "img_encoder.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6jXNVO_NTQK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "prep_from_tensor = img_encoder.preprocess_input(images)\n",
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_kiwfVRNTQK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "prep_from_img.shape, prep_from_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6-sv-21NTQK",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.all(prep_from_img.to(get_device()) == prep_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zH-kZ7jNTQL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.norm(prep_from_img.to(get_device()) - prep_from_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PKc_ZKZNTQL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "to_pil(prep_from_img.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4CynljoNTQL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "to_pil(prep_from_tensor.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVB9yOV0NTQL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "to_pil((prep_from_img - prep_from_tensor).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtfIf3bLNTQL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if batch_size > 1:\n",
    "    grid = torchvision.utils.make_grid([prep_from_img, prep_from_tensor], nrow=2, normalize=False, range=(-1, 1))\n",
    "    img = to_pil(grid)\n",
    "else:\n",
    "    img = to_pil(prep_from_img.squeeze(0))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OpFxAEVNTQL",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "grid = torchvision.utils.make_grid([prep_from_img.squeeze(), prep_from_tensor.squeeze()], nrow=2, normalize=False, range=(-1, 1))\n",
    "img = to_pil(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
