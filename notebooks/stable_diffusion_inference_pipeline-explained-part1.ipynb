{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook intends to show how to use the developed API to construct a `StableDiffusion` object and generate an image from a textual prompt.\n",
    "\n",
    "---\n",
    "\n",
    "We started noticing that, in the reference codebase, running any part of the stable diffusion model required the initialization and loading of the entire `LatentDiffusion` module, which loads the stable diffusion models' checkpoints.\n",
    "\n",
    "The `LatentDiffusion` module is composed mainly by three submodules: `UNetModel`, `Autoencoder` and `CLIPTextEmbedder`. It makes sense to use these submodules individually, and we wanted to increase the pipeline modularity, in order for it to support individual runs. For instance, the `CLIPTextEmbedder` turns textual prompts into tensors in an embedding space. It makes perfect sense to embed a number of textual prompts, then run the rest of the pipeline over these embeddings. Heavier parts of the `LatentDiffusion` model, such as the UNet and the autoencoder aren't needed to embed the prompts. Hence the first goal was to add support for the individual usage of the submodules that 'made sense' to be used individually.\n",
    "\n",
    "The second goal was to avoid downloads during runtime altogether, while minimizing usage of external libraries. These downloads were occurring mainly due to the `transformers` library, that was being used to load/download the `openai/clip-vit-large-patch14` model and construct the text embedder. \n",
    "\n",
    "The third goal was to add support to `.safetensors`. Since `torch` still doesn't support it natively, and the very process of saving and loading being different, that was quite troublesome. To begin with, with `safetensors` the serialization is only at tensors level, so you can only save/load dictionaries of tensors, which are in general weights or state dicts; with pickle, you can save the very Python object, so that when you load it *you already have an instance of it*. That's important because *you need an instance of the model you are trying to load weights into*. And if you don't have the code for the object (in general, the code for a `nn.Module` that instantiates a module with a state dict compatible with what you are loading into object), you won't be able to use `.safetensors`. And we didn't had the codes for the submodels the `CLIPTextEmbedder` has, since they were coming from `transformers` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kk-digital/kcg-ml-sd1p4\n",
    "%cd kcg-ml-sd1p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "print(os.path.abspath(base_directory))\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import configparser\n",
    "import safetensors\n",
    "from stable_diffusion.utils_backend import *\n",
    "from stable_diffusion.utils_image import *\n",
    "from stable_diffusion.utils_model import *\n",
    "from stable_diffusion.utils_logger import *\n",
    "\n",
    "from stable_diffusion.constants import IODirectoryTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_device()\n",
    "config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())\n",
    "config.read(os.path.join(base_directory, \"config.ini\"))\n",
    "config['BASE']['BASE_DIRECTORY'] = base_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = IODirectoryTree(base_io_directory_prefix = config[\"BASE\"].get('base_io_directory_prefix'), base_directory=base_directory)\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.create_directory_tree_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using `transformers` for the CLIP models.\n",
    "\n",
    "On a first run, since we don't have the required model on cache, the next cell would normally download the pretrained tokenizer from `openai/clip-vit-large-patch14` on Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we have the tokenizer files (are very light) in our repo, so we load from it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(pt.tokenizer_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you save it\n",
    "# sd_savepath = os.path.join(pt.sd_model_dir, \"clip_\")\n",
    "# tokenizer.save_pretrained(sd_savepath+\"tokenizer\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, if we didn't have the required configuration file on cache, the next cell would normally download the `CLIPTextModel` config file from `openai/clip-vit-large-patch14` on Huggingface. That is needed for us to initialize an empty `CLIPTextModel` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextConfig, CLIPTextModel\n",
    "\n",
    "#fetch config file from huggingface and save it to the model folder\n",
    "# config = CLIPTextConfig.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# config.save_pretrained(pt.text_model_path)\n",
    "# config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have that config file in our repo, so we can load it from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CLIPTextConfig.from_pretrained(pt.text_model_path, local_files_only=True)\n",
    "# config = CLIPTextConfig.from_pretrained('../input/model/clip/text_embedder/text_model/config.json')\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can finally instantiate a `CLIPTextModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = CLIPTextModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.save_pretrained(pt.text_model_path, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test load\n",
    "# text_model = CLIPTextModel.from_pretrained(pt.text_model_path, local_files_only=True, use_safetensors=True).eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally can instantiate a text embedder without loading any weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedder = CLIPTextEmbedder(pt, device=DEVICE, tokenizer = tokenizer, transformer=text_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedder.to(text_embedder.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, at this point we should be able to embed a prompt, albeit badly, because we started the CLIPTextModel with no weights, the configuration alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedder('A great sword')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we haven't done the process of creating the submodels instances, we would have, instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_text_embedder = CLIPTextEmbedder(pt, device=DEVICE, tokenizer = None, transformer= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_text_embedder.to(not_text_embedder.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, obviously, our forward wouldn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_text_embedder('A great sword')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redo the text embedder, but now loading the saved submodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedder = CLIPTextEmbedder(pt, device=DEVICE, tokenizer = None, transformer= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still empty\n",
    "text_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedder.load_submodels(tokenizer_path = pt.tokenizer_path, transformer_path = pt.text_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also save our submodels to disk for later use\n",
    "# text_embedder.save_submodels(tokenizer_path=pt.tokenizer_path, text_model_path=pt.text_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create an instance for two other submodules, `UNetModel` and `Autoencoder`. Those submodules should be easier to initialize since we have the `nn.Module` objects defined, and can avoid `transformers` entirely.\n",
    "\n",
    "The `Autoencoder` is also composed of two submodules that are actually useful individually, `Encoder` and `Decoder`. Let's start instantiating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_diffusion.utils.model import initialize_encoder\n",
    "from stable_diffusion.model.vae import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_diffusion.utils.model import initialize_decoder\n",
    "from stable_diffusion.model.vae import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = initialize_decoder(device=DEVICE)\n",
    "decoder = Decoder(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_diffusion.utils.model import initialize_autoencoder\n",
    "from stable_diffusion.model.vae import Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder = initialize_autoencoder(device=DEVICE, encoder=encoder, decoder=decoder)\n",
    "autoencoder = Autoencoder(device=DEVICE, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have an untrained autoencoder. Now we just need the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_diffusion.model.unet import UNetModel\n",
    "# from stable_diffusion.utils.model import initialize_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = UNetModel(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to build a model with the same structure that the checkpoint we are going to use (by default, `runwayml/stable-diffusion-v1-5`), so the weights get properly mapped. This model is called `LatentDiffusion`. We also have a `initialize_latent_diffusion` function, which I will omit since it's a bit longer than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_diffusion import LatentDiffusion\n",
    "# from stable_diffusion.utils.model import initialize_latent_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_diffusion = LatentDiffusion(\n",
    "                            autoencoder=autoencoder,\n",
    "                            clip_embedder=text_embedder,\n",
    "                            unet_model=unet_model,\n",
    "                            device=DEVICE\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with section(f\"stable diffusion checkpoint loading, from {pt.checkpoint_path}\"):\n",
    "    stable_diffusion_checkpoint = safetensors.torch.load_file(pt.checkpoint_path, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push them weights into dat model, ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with section('model state loading'):\n",
    "    missing_keys, extra_keys = latent_diffusion.load_state_dict(stable_diffusion_checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common that some weights don't get mapped perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extra_keys)\n",
    "print(len(extra_keys))\n",
    "print(missing_keys)\n",
    "print(len(missing_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we have a fully loaded latent diffusion model. To actually perform the 'stable diffusion', which is actually a kind of latent diffusion model, we need yet another class, the `StableDiffusion`. Roughly speaking, the `StableDiffusion` class uses the `LatentDiffusion` model in a specific way to denoise a random sample from the latent space. It uses a diffusion process for that, hence 'latent diffusion'. What defines this process, i.e, how to use the `LatentDiffusion` model to denoise a random sampling is a sampler. That's what gets added into the `StableDiffusion` class. Besides that, it provides a unified interface for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_diffusion import StableDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion = StableDiffusion(device=DEVICE, model = latent_diffusion, ddim_steps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'A cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with section('sampling...'):\n",
    "    image_tensor = stable_diffusion.generate_images(prompt = prompt, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil(image_tensor.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish this notebook by saving all the relevant submodels to disk, with their weights loaded in. What we did: we broke the `v1-5...` checkpoint, a big file, into one checkpoint for each model, so now we can load the weights that were contained in the checkpoint more modularly. We will start part 2 by redoing the process of assembling a `StableDiffusion` instance by loading the checkpoints for the saved models, instead of loading the checkpoint for the `LatentDiffusion` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first stage model is the autoencoder; let's save it's submodels\n",
    "stable_diffusion.model.first_stage_model.save_submodels(encoder_path = pt.encoder_path, decoder_path = pt.decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the autoencoder itself also has parameters, so we also need to save it; but let's unload it's submodels first\n",
    "stable_diffusion.model.first_stage_model.unload_submodels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save the unloaded autoencoder\n",
    "stable_diffusion.model.first_stage_model.save(autoencoder_path=pt.autoencoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cond stage is the conditioning stage: the CLIPTextEmbedder model. let's save it's submodels too\n",
    "stable_diffusion.model.cond_stage_model.save_submodels(tokenizer_path = pt.tokenizer_path, transformer_path = pt.text_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion.model.cond_stage_model.unload_submodels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the unloaded CLIPTextEmbedder\n",
    "stable_diffusion.model.cond_stage_model.save(text_embedder_path=pt.text_embedder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the UNet model\n",
    "stable_diffusion.model.model.diffusion_model.save(unet_path=pt.unet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `LatentDiffusion` also has parameters, so we should save it as well, but only after unloading the submodels.\n",
    "stable_diffusion.model.unload_submodels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the unloaded latent diffusion model\n",
    "stable_diffusion.model.save(latent_diffusion_path=pt.latent_diffusion_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in part 2, let's rebuild a `StableDiffusion` class, with the saved submodels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
