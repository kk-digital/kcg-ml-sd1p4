{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqqfyS3ziEFv"
   },
   "outputs": [],
   "source": [
    "ENV_TYPE = \"TEST\"\n",
    "\n",
    "if(ENV_TYPE != \"TEST\"):\n",
    "  !git clone \"https://github.com/kk-digital/kcg-ml-sd1p4.git\"\n",
    "  %cd kcg-ml-sd1p4\n",
    "  !pip3 install -r requirements.txt\n",
    "  exit()\n",
    "  base_directory = \"./\"\n",
    "else:\n",
    "  base_directory = \"../\"\n",
    "\n",
    "# Magical check for fixing all of our directory issues\n",
    "import subprocess\n",
    "output = subprocess.check_output([\"pwd\"], universal_newlines=True)\n",
    "if \"notebooks\" in output:\n",
    "    %cd ..\n",
    "del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_oZIySUiEFy"
   },
   "outputs": [],
   "source": [
    "!python3 ./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HshPVm2XiEFz"
   },
   "outputs": [],
   "source": [
    "!python3 ./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHRANLRTiEF0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "from torchvision.transforms import ToPILImage\n",
    "from os.path import join\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
    "output_directory = join(output_base_dir, \"demo/\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_directory)\n",
    "except Exception as e:\n",
    "    print(e, \"\\n\", \"Creating the path...\")\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "from stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.utils_backend import *\n",
    "from stable_diffusion.utils_image import *\n",
    "from stable_diffusion.utils_model import *\n",
    "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
    "\n",
    "from stable_diffusion.model_paths import *\n",
    "from configs.model_config import ModelPathConfig\n",
    "\n",
    "device = get_device()\n",
    "to_pil = lambda image: ToPILImage()(torch.clamp((image + 1.0) / 2.0, min=0.0, max=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1KHVOT-iEF0"
   },
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "sys.path.insert(0, base_dir)\n",
    "\n",
    "batch_size = 1\n",
    "model_config = ModelPathConfig()\n",
    "pt = IODirectoryTree(model_config)\n",
    "\n",
    "print(\n",
    "    pt.autoencoder\n",
    ")  # should be `.../kcg-ml-sd1p4/input/model/autoencoder/autoencoder.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebrOZ_G7iEF1"
   },
   "outputs": [],
   "source": [
    "# define the logistic distribution\n",
    "def logistic_distribution(loc, scale):\n",
    "    base_distribution = torch.distributions.Uniform(0, 1)\n",
    "    transforms = [\n",
    "        torch.distributions.transforms.SigmoidTransform().inv,\n",
    "        torch.distributions.transforms.AffineTransform(loc=loc, scale=scale),\n",
    "    ]\n",
    "    logistic = torch.distributions.TransformedDistribution(\n",
    "        base_distribution, transforms\n",
    "    )\n",
    "    return logistic\n",
    "\n",
    "\n",
    "noise_fn = (\n",
    "    lambda shape, device=device: logistic_distribution(loc=0.0, scale=0.5)\n",
    "    .sample(shape)\n",
    "    .to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaR7OLjdiEF2"
   },
   "outputs": [],
   "source": [
    "# Load Stable Diffusion\n",
    "DEVICE = get_device()\n",
    "N_STEPS = 25\n",
    "\n",
    "\n",
    "sd = StableDiffusion(device=DEVICE, n_steps=N_STEPS)\n",
    "sd.quick_initialize()\n",
    "sd.model.load_submodel_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7etnV6RiEF4"
   },
   "outputs": [],
   "source": [
    "# choose a temperature for the sampling (in general higher means more diversity but less quality) and generate an image, then save it and show it\n",
    "# temperature only makes any difference if `ddim_eta` is different from zero\n",
    "\n",
    "temperature = 1.0\n",
    "imgs = sd.generate_images(\n",
    "    prompt=\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\",\n",
    "    negative_prompt=\"Ugly, weird\",\n",
    "    seed=2982,\n",
    "    noise_fn=noise_fn,\n",
    "    temperature=temperature,\n",
    ")\n",
    "save_images(\n",
    "    imgs,\n",
    "    join(\n",
    "        output_directory,\n",
    "        f\"test_sample_temp{temperature:.3f}_eta{sd.ddim_eta:.3f}.png\",\n",
    "    ),\n",
    ")\n",
    "to_pil(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e725o84iEF5"
   },
   "outputs": [],
   "source": [
    "# change the ddim_eta parameter and generate another image, then save it and show it\n",
    "sd.ddim_eta = 0.1\n",
    "temperature = 1.0\n",
    "imgs = sd.generate_images(\n",
    "    prompt=\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\",\n",
    "    negative_prompt=\"Ugly, weird\",\n",
    "    seed=2982,\n",
    "    noise_fn=noise_fn,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "save_images(\n",
    "    imgs,\n",
    "    join(\n",
    "        output_directory,\n",
    "        f\"test_sample_temp{temperature:.3f}_eta{sd.ddim_eta:.3f}.png\",\n",
    "    ),\n",
    ")\n",
    "to_pil(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "podd9iAjiEF5"
   },
   "outputs": [],
   "source": [
    "# higher `ddim_eta`s imply higher noise levels\n",
    "sd.ddim_eta = 0.5\n",
    "temperature = 1.0\n",
    "imgs = sd.generate_images(\n",
    "    prompt=\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\",\n",
    "    negative_prompt=\"Ugly, weird\",\n",
    "    seed=2982,\n",
    "    noise_fn=noise_fn,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "save_images(\n",
    "    imgs,\n",
    "    join(\n",
    "        output_directory,\n",
    "        f\"test_sample_temp{temperature:.3f}_eta{sd.ddim_eta:.3f}.png\",\n",
    "    ),\n",
    ")\n",
    "to_pil(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTytYomjiEF5"
   },
   "outputs": [],
   "source": [
    "# and so do higher temperatures\n",
    "sd.ddim_eta = 0.5\n",
    "temperature = 1.8\n",
    "imgs = sd.generate_images(\n",
    "    prompt=\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\",\n",
    "    negative_prompt=\"Ugly, weird\",\n",
    "    seed=2982,\n",
    "    noise_fn=noise_fn,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "save_images(\n",
    "    imgs,\n",
    "    join(\n",
    "        output_directory,\n",
    "        f\"test_sample_temp{temperature:.3f}_eta{sd.ddim_eta:.3f}.png\",\n",
    "    ),\n",
    ")\n",
    "to_pil(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khY4lxI1iEF5"
   },
   "outputs": [],
   "source": [
    "# we can check how the images change with the ddim_eta parameter\n",
    "temperature = 1.0\n",
    "images = []\n",
    "eta_steps = 5\n",
    "eta_0 = 0.0\n",
    "for i in range(eta_steps):\n",
    "    ddim_eta = eta_0 + i * 0.1\n",
    "    sd.ddim_eta = ddim_eta\n",
    "    imgs = sd.generate_images(\n",
    "        prompt=\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\",\n",
    "        negative_prompt=\"Ugly, weird\",\n",
    "        seed=2982,\n",
    "        noise_fn=noise_fn,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    print(imgs.shape)\n",
    "    images.append(imgs)\n",
    "images = torch.cat(images, dim=0)\n",
    "grid = torchvision.utils.make_grid(\n",
    "    images, normalize=False, range=(-1, 1), scale_each=True, pad_value=0\n",
    ")\n",
    "grid_img = to_pil(grid)\n",
    "grid_img.save(\n",
    "    join(\n",
    "        output_directory,\n",
    "        f\"test_grid_temp{temperature:.3f}_eta{eta_0:.3f}-{sd.ddim_eta:.3f}.png\",\n",
    "    )\n",
    ")\n",
    "grid_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xo_POaP_iEF5"
   },
   "outputs": [],
   "source": [
    "# or we can check how the images change with the temperature alone\n",
    "temperature = 1.0\n",
    "sd.ddim_eta = 0.1\n",
    "images = []\n",
    "temp_steps = 5\n",
    "for i in range(temp_steps):\n",
    "    temperature += 0.1\n",
    "    imgs = sd.generate_images(\n",
    "        prompt=\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\",\n",
    "        negative_prompt=\"Ugly, weird\",\n",
    "        seed=2982,\n",
    "        noise_fn=noise_fn,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    print(imgs.shape)\n",
    "    images.append(imgs)\n",
    "images = torch.cat(images, dim=0)\n",
    "grid = torchvision.utils.make_grid(\n",
    "    images,\n",
    "    normalize=False,\n",
    "    nrow=temp_steps,\n",
    "    range=(-1, 1),\n",
    "    scale_each=True,\n",
    "    pad_value=0,\n",
    ")\n",
    "grid_img = to_pil(grid)\n",
    "grid_img.save(\n",
    "    join(\n",
    "        output_directory,\n",
    "        f\"test_grid_temp{temperature-temp_steps*0.1:.3f}-{temperature:.3f}_eta{sd.ddim_eta:.3f}.png\",\n",
    "    )\n",
    ")\n",
    "grid_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LpiLPxZiEF6"
   },
   "outputs": [],
   "source": [
    "# or we can vary both things simultaneously (ddim_eta on y-axis, temperature on x-axis; it increases from left to right and from top to bottom)\n",
    "grid_side = 2\n",
    "temperature = 1.0\n",
    "ddim_eta = 0.1\n",
    "grid = []\n",
    "# rows = []\n",
    "for i in range(grid_side + 1):\n",
    "    temperature += 0.2\n",
    "    for j in range(grid_side + 1):\n",
    "        sd.ddim_eta = ddim_eta + j * 0.1\n",
    "        imgs = sd.generate_images(\n",
    "            prompt=\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\",\n",
    "            negative_prompt=\"Ugly, weird\",\n",
    "            seed=2982,\n",
    "            noise_fn=noise_fn,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        # rows.append(imgs)\n",
    "        grid.append(imgs)\n",
    "    # grid.append(torch.cat(rows, dim=0))\n",
    "\n",
    "tensor_grid = torch.cat(grid, dim=0)\n",
    "tensor_grid.shape\n",
    "grid = torchvision.utils.make_grid(\n",
    "    tensor_grid,\n",
    "    nrow=grid_side + 1,\n",
    "    normalize=False,\n",
    "    range=(-1, 1),\n",
    "    scale_each=True,\n",
    "    pad_value=0,\n",
    ")\n",
    "grid_img = to_pil(grid)\n",
    "grid_img.save(\n",
    "    join(\n",
    "        output_directory,\n",
    "        f\"test_grid_temp{temperature-grid_side*0.2:.3f}-{temperature:.3f}_eta{ddim_eta:.3f}-{sd.ddim_eta:.3f}.png\",\n",
    "    )\n",
    ")\n",
    "grid_img"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
