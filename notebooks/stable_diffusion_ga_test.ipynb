{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "metadata": {
        "id": "bKD3sXj6E72o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chad_score import ChadPredictor\n",
        "import torch\n",
        "from typing import List\n",
        "import hashlib\n",
        "import json\n",
        "import math\n",
        "\n",
        "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder\n",
        "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
        "from stable_diffusion import StableDiffusion\n",
        "from stable_diffusion.constants import ModelsPathTree\n",
        "from stable_diffusion.utils.utils import (\n",
        "    get_device,\n",
        "    get_memory_status,\n",
        "    to_pil,\n",
        "    save_image_grid,\n",
        "    show_image_grid,\n",
        ")\n",
        "\n",
        "from os.path import join\n",
        "import os, sys\n",
        "\n",
        "base_dir = \"./\"\n",
        "sys.path.insert(0, base_dir)\n",
        "\n",
        "# Variables\n",
        "SEED = 1337\n",
        "NOISE_MAX_MULTIPLIER = 0.5\n",
        "BATCH_SIZE = 1\n",
        "POPULATION_SIZE = 12\n",
        "GEN_IMAGE_N_GENERATIONS = 50\n",
        "N_STEPS = 10\n",
        "EMBEDDED_PROMPTS_DIR = os.path.abspath(join(base_dir, \"./input/embedded_prompts/\"))\n",
        "OUTPUT_DIR = os.path.abspath(\n",
        "    os.path.join(base_dir, \"./output/ga/\")\n",
        ")\n",
        "IMAGES_DIR = os.path.abspath(join(OUTPUT_DIR, \"images/\"))\n",
        "FEATURES_DIR = os.path.abspath(join(OUTPUT_DIR, \"features/\"))\n",
        "\n",
        "NULL_PROMPT = \"\"\n",
        "PROMPT = [\n",
        "    \"Bedroom interior, mid century modern, retro, vintage, designer furniture made of wood and plastic, concrete nightstand, wood walls, potted plant on a shelf, large window, outdoor cityscape view, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
        "    \"Dining room interior, mid century modern, retro, vintage, designer dining set made of wood and plastic, concrete floor, wood paneling, flower vase on a sideboard, large window overlooking a garden, outdoor countryside landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
        "    \"Office interior, mid century modern, retro, vintage, designer desk made of wood and plastic, concrete bookshelf, wood-paneled walls, potted plant on a window sill, large window with a view of the ocean, outdoor coastal landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
        "    \"Kitchen interior, mid century modern, retro, vintage, designer kitchen island made of wood and plastic, concrete countertops, wood cabinets, flowerpot on a windowsill, large window overlooking a lush garden, outdoor tropical landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
        "    \"Study room interior, mid century modern, retro, vintage, designer study desk made of wood and plastic, concrete floor, wood-paneled walls, potted bonsai tree on a shelf, large window with a view of the mountains, outdoor snowy landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
        "    \"Bathroom interior, mid century modern, retro, vintage, designer bathtub and sink made of wood and plastic, concrete walls, potted fern on a ledge, large window overlooking a tranquil lake, outdoor lakeside landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
        "    \"Studio interior, mid century modern, retro, vintage, designer studio set-up made of wood and plastic, concrete flooring, wood accent walls, flowerpot on a table, large window with a view of a futuristic cityscape, outdoor urban landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
        "    \"Lounge interior, mid century modern, retro, vintage, designer lounge chairs made of wood and plastic, concrete coffee table, wood-paneled walls, potted cactus on a shelf, large window overlooking a desert landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\"\n",
        "]\n",
        "\n",
        "# DEVICE = input(\"Set device: 'cuda:i' or 'cpu'\")\n",
        "DEVICE = None\n",
        "DEVICE = get_device(DEVICE)\n",
        "\n",
        "\n",
        "print(EMBEDDED_PROMPTS_DIR)\n",
        "print(OUTPUT_DIR)\n",
        "print(IMAGES_DIR)\n",
        "print(FEATURES_DIR)\n",
        "pt = ModelsPathTree(base_directory=base_dir)\n",
        "\n",
        "os.makedirs(EMBEDDED_PROMPTS_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "os.makedirs(FEATURES_DIR, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxVwKkBrW4_8",
        "outputId": "a0407ed0-1e59-45bc-e0a1-e34b85bc5951"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: `device` is None. Using device  cuda:0.\n",
            "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
            "/root/repo-personal/kcg-ml-sd1p4/input/embedded_prompts\n",
            "/root/repo-personal/kcg-ml-sd1p4/output/ga\n",
            "/root/repo-personal/kcg-ml-sd1p4/output/ga/images\n",
            "/root/repo-personal/kcg-ml-sd1p4/output/ga/features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sha256(tensor):\n",
        "    if tensor.device == \"cpu\":\n",
        "        tensor_bytes = tensor.numpy().tobytes()  # Convert tensor to a byte array\n",
        "    else:\n",
        "        tensor_bytes = tensor.cpu().numpy().tobytes()  # Convert tensor to a byte array\n",
        "    sha256_hash = hashlib.sha256(tensor_bytes)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "\n",
        "def embed_and_save_prompts(prompts: list, null_prompt=NULL_PROMPT):\n",
        "    null_prompt = null_prompt\n",
        "    prompts = prompts\n",
        "\n",
        "    clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
        "    clip_text_embedder.load_submodels(**pt.embedder_submodels)\n",
        "\n",
        "    null_cond = clip_text_embedder(null_prompt)\n",
        "    torch.save(null_cond, join(EMBEDDED_PROMPTS_DIR, \"null_cond.pt\"))\n",
        "    print(\n",
        "        \"Null prompt embedding saved at: \",\n",
        "        f\"{join(EMBEDDED_PROMPTS_DIR, 'null_cond.pt')}\",\n",
        "    )\n",
        "\n",
        "    embedded_prompts = clip_text_embedder(prompts)\n",
        "    torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_prompts.pt\"))\n",
        "\n",
        "    print(\n",
        "        \"Prompts embeddings saved at: \",\n",
        "        f\"{join(EMBEDDED_PROMPTS_DIR, 'embedded_prompts.pt')}\",\n",
        "    )\n",
        "\n",
        "    get_memory_status()\n",
        "    clip_text_embedder.to(\"cpu\")\n",
        "    del clip_text_embedder\n",
        "    torch.cuda.empty_cache()\n",
        "    get_memory_status()\n",
        "\n",
        "    return embedded_prompts, null_cond\n",
        "\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    import numpy as np\n",
        "\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2 == 0] = 1\n",
        "    return a / np.expand_dims(l2, axis)"
      ],
      "metadata": {
        "id": "Dd_21FmhHA4B"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for each prompt\n",
        "embedded_prompts, null_prompt = embed_and_save_prompts(PROMPT)\n",
        "embedding = embedded_prompts\n",
        "num_images = embedding.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzSdG4VCHCS5",
        "outputId": "b6e75246-492f-4f61-dd91-5918c239ef41"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: `device` is None. Using device  cuda:0.\n",
            "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
            "INFO: Device given. Using device cuda:0.\n",
            "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
            "Null prompt embedding saved at:  /root/repo-personal/kcg-ml-sd1p4/input/embedded_prompts/null_cond.pt\n",
            "Prompts embeddings saved at:  /root/repo-personal/kcg-ml-sd1p4/input/embedded_prompts/embedded_prompts.pt\n",
            "Total: 24259 MiB\n",
            "Free: 14544 MiB\n",
            "Used: 9715 MiB\n",
            "Total: 24259 MiB\n",
            "Free: 15700 MiB\n",
            "Used: 8559 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate noise and add to the embedded prompt\n",
        "def add_noise_to_embeds(embedded_prompts, noise_multiplier):\n",
        "    embedded_prompts.mean(dim=2), embedded_prompts.std(dim=2)\n",
        "    noise = torch.normal(mean=embedded_prompts.mean(dim=2), std=embedded_prompts.std(dim=2))\n",
        "    dist = torch.distributions.normal.Normal(\n",
        "        loc=embedded_prompts.mean(dim=2), scale=embedded_prompts.std(dim=2)\n",
        "    )\n",
        "    noise = dist.sample(sample_shape=torch.Size([768])).permute(1, 0, 2).permute(0, 2, 1)\n",
        "    noise.shape\n",
        "\n",
        "    generator = torch.Generator(device=DEVICE).manual_seed(SEED) if SEED is not None else torch.Generator(device=DEVICE)\n",
        "\n",
        "    dist = torch.distributions.normal.Normal(\n",
        "        loc=embedded_prompts.mean(dim=2), scale=embedded_prompts.std(dim=2)\n",
        "    )\n",
        "    noise = dist.sample(sample_shape=torch.Size([768])).permute(1, 0, 2).permute(0, 2, 1)\n",
        "\n",
        "    embedding_e = embedded_prompts + noise_multiplier * noise\n",
        "\n",
        "    return embedding_e"
      ],
      "metadata": {
        "id": "XJmQRUQaO_gE"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Stable Diffusion\n",
        "sd = StableDiffusion(device=DEVICE, n_steps=N_STEPS)\n",
        "sd.quick_initialize().load_autoencoder(**pt.autoencoder).load_decoder(**pt.decoder)\n",
        "sd.model.load_unet(**pt.unet)\n",
        "\n",
        "# Load chadscore and clip\n",
        "import clip\n",
        "\n",
        "# Test calculate chadscore\n",
        "chad = ChadPredictor(768)\n",
        "chad.load_state_dict(torch.load(\"/input/models/aesthetic_scorer/sac+logos+ava1-l14-linearMSE.pth\") )\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "  chad.to(\"cuda\")\n",
        "chad.eval()\n",
        "image_features_clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)  #RN50x64"
      ],
      "metadata": {
        "id": "LYH6CWsOMbR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51cd7e1b-27b7-4cc4-9652-0406b46e78bd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Device given. Using device cuda:0.\n",
            "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
            "WARNING: `LatentDiffusion` model is `None` given. Initialize one with the appropriate method.\n",
            "INFO: Device given. Using device cuda:0.\n",
            "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
            "Autoencoder loaded from: /root/repo-personal/kcg-ml-sd1p4/input/model/autoencoder/autoencoder.ckpt\n",
            "Decoder loaded from: /root/repo-personal/kcg-ml-sd1p4/input/model/autoencoder/decoder.ckpt\n",
            "INFO: `device` is None. Using device  cuda:0.\n",
            "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_from_embeddings(embedded_prompts_array, null_prompt):\n",
        "  # 'embedded_prompts_array' is the array of shape (i, 77, 768)\n",
        "  image = sd.generate_images_from_embeddings(\n",
        "      seed=SEED, embedded_prompt=embedded_prompts_array[i:i+1], null_prompt=null_prompt\n",
        "  )\n",
        "  return image"
      ],
      "metadata": {
        "id": "kqPwnmEYeXod"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pygad\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Function to calculate the chad score for batch of images\n",
        "def calculate_chad_score(ga_instance, solution, solution_idx):\n",
        "  # Convert the solution back to the original shape (1, 77, 768)\n",
        "  solution_reshaped = solution.reshape(1, 77, 768)\n",
        "\n",
        "  # Convert the numpy array to a PyTorch tensor\n",
        "  solution_reshaped = torch.tensor(solution_reshaped, dtype=torch.float32)\n",
        "\n",
        "  # Copy the tensor to CUDA device if 'device' is 'cuda'\n",
        "  if device == 'cuda':\n",
        "    solution_reshaped = solution_reshaped.to(device)\n",
        "\n",
        "  # Generate an image using the solution\n",
        "  image = generate_images_from_embeddings(solution_reshaped, null_prompt)\n",
        "\n",
        "  pil_image = to_pil(image[0])  # Convert to (height, width, channels)\n",
        "  unsqueezed_image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    image_features = image_features_clip_model.encode_image(unsqueezed_image)\n",
        "\n",
        "    im_emb_arr = normalized(image_features.cpu().detach().numpy() )\n",
        "    prediction = chad(torch.from_numpy(im_emb_arr).to(device).type(torch.cuda.FloatTensor))\n",
        "    chad_score = prediction.item()\n",
        "  return chad_score\n",
        "\n",
        "# Define the GA loop function\n",
        "def genetic_algorithm_loop(sd, embedded_prompts, null_prompt, generations=10, population_size=POPULATION_SIZE, mutation_rate=0.4, num_parents_mating=2):\n",
        "    # Move the 'embedded_prompts' tensor to CPU memory\n",
        "    embedded_prompts_cpu = embedded_prompts.cpu()\n",
        "\n",
        "    # Reshape the 'embedded_prompts' tensor to a 2D numpy array\n",
        "    embedded_prompts_array = embedded_prompts_cpu.detach().numpy()\n",
        "    num_individuals = embedded_prompts_array.shape[0]\n",
        "    num_genes = embedded_prompts_array.shape[1] * embedded_prompts_array.shape[2]\n",
        "    embedded_prompts_list = embedded_prompts_array.reshape(num_individuals, num_genes).tolist()\n",
        "\n",
        "    # Initialize the GA\n",
        "    ga_instance = pygad.GA(num_generations=generations,\n",
        "                           num_parents_mating=num_parents_mating,\n",
        "                           fitness_func=calculate_chad_score,\n",
        "                           sol_per_pop=population_size,\n",
        "                           num_genes=num_genes,\n",
        "                           initial_population=embedded_prompts_list,\n",
        "                           mutation_percent_genes=mutation_rate*100)\n",
        "\n",
        "    # Run the GA loop\n",
        "    for generation in range(ga_instance.num_generations):\n",
        "\n",
        "        # Mutate each individual in the population with noise\n",
        "        #for i in range(ga_instance.population.shape[0]):\n",
        "           # Extract the individual's solution from the population\n",
        "        #    solution = ga_instance.population[i]\n",
        "\n",
        "            # Add noise to the solution using the 'add_noise_to_embeds' function\n",
        "            #noisy_solution = add_noise_to_embeds(solution, random.uniform(0, NOISE_MAX_MULTIPLIER))\n",
        "            #ga_instance.population[i] = noisy_solution.reshape(-1)\n",
        "\n",
        "        # Get the best solution from the GA\n",
        "        best_solution, best_solution_fitness, best_solution_idx = ga_instance.best_solution()\n",
        "\n",
        "        print(\"Generation:\", generation)\n",
        "        print(\"The current best chadscore for our youngsters is:\", best_solution_fitness)\n",
        "\n",
        "        # Save images every N generations (ex. 10)\n",
        "        if generation % 1 == 0:\n",
        "          # Convert the solution back to the original shape (1, 77, 768)\n",
        "          solution_reshaped = best_solution.reshape(1, 77, 768)\n",
        "          solution_reshaped = torch.tensor(solution_reshaped, dtype=torch.float32)\n",
        "\n",
        "          # Copy the tensor to CUDA device if 'device' is 'cuda'\n",
        "          if device == 'cuda':\n",
        "            solution_reshaped = solution_reshaped.to(device)\n",
        "\n",
        "          image = generate_images_from_embeddings(solution_reshaped, null_prompt)\n",
        "          pil_image = to_pil(image[0])\n",
        "          filename=f\"{IMAGES_DIR}/{generation}.png\"\n",
        "          pil_image.show()\n",
        "          pil_image.save(filename)\n",
        "\n",
        "    # Get the final best solution and images\n",
        "    best_solution, best_solution_fitness = ga_instance.best_solution()\n",
        "    best_solution = best_solution.reshape(1, 77, 768)  # Reshape the best solution to the correct shape\n",
        "    final_image = sd.generate_images_from_embeddings(embedded_prompt=best_solution, null_prompt=null_prompt)\n",
        "\n",
        "    # Save the final best solution images\n",
        "    final_filename=f\"{IMAGES_DIR}/final.png\"\n",
        "    pil_final = to_pil(final_image[0])\n",
        "    pil_final.show()\n",
        "    pil_final.save(final_filename)\n",
        "\n",
        "    return best_solution\n",
        "\n",
        "# Call the GA loop function with your initialized StableDiffusion model\n",
        "best_solution = genetic_algorithm_loop(sd, embedded_prompts, null_prompt)\n",
        "\n",
        "print(\"Best solution found!\")\n",
        "\n",
        "torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_final_solution.pt\"))\n",
        "print(\"Saving solution...\")"
      ],
      "metadata": {
        "id": "3VxFRyYQK_oq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "ff985444-3324-42ec-e034-fcc8ca7bafe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,154.32ms</span>\n",
              "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,176.93ms</span>\n",
              "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,149.66ms</span>\n",
              "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,169.34ms</span>\n",
              "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,174.42ms</span>\n",
              "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,174.08ms</span>\n",
              "      Sample<span style=\"color: #D160C4\">   50%</span><span style=\"color: #208FFB\">\t1,195.48ms</span></pre>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean unused loaded models\n",
        "del preprocess, image_features_clip_model, sd"
      ],
      "metadata": {
        "id": "UL-xzoZT8qvW"
      },
      "execution_count": 50,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}