{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kk-digital/kcg-ml-sd1p4\n",
    "%cd kcg-ml-sd1p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKD3sXj6E72o"
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxVwKkBrW4_8",
    "outputId": "a0407ed0-1e59-45bc-e0a1-e34b85bc5951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `device` is None. Using device  cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
      "/root/repo-personal/kcg-ml-sd1p4/input/embedded_prompts\n",
      "/root/repo-personal/kcg-ml-sd1p4/output/ga\n",
      "/root/repo-personal/kcg-ml-sd1p4/output/ga/images\n",
      "/root/repo-personal/kcg-ml-sd1p4/output/ga/features\n"
     ]
    }
   ],
   "source": [
    "from chad_score import ChadPredictor\n",
    "import torch\n",
    "from typing import List\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "\n",
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder\n",
    "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
    "from stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.constants import ModelsPathTree\n",
    "from stable_diffusion.utils.utils import (\n",
    "    get_device,\n",
    "    get_memory_status,\n",
    "    to_pil,\n",
    "    save_image_grid,\n",
    "    show_image_grid,\n",
    ")\n",
    "\n",
    "from os.path import join\n",
    "import os, sys\n",
    "\n",
    "base_dir = \"./\"\n",
    "sys.path.insert(0, base_dir)\n",
    "\n",
    "# Variables\n",
    "SEED = 1337\n",
    "NOISE_MAX_MULTIPLIER = 0.5\n",
    "BATCH_SIZE = 1\n",
    "POPULATION_SIZE = 12\n",
    "GEN_IMAGE_N_GENERATIONS = 50\n",
    "N_STEPS = 10\n",
    "EMBEDDED_PROMPTS_DIR = os.path.abspath(join(base_dir, \"./input/embedded_prompts/\"))\n",
    "OUTPUT_DIR = os.path.abspath(\n",
    "    os.path.join(base_dir, \"./output/ga/\")\n",
    ")\n",
    "IMAGES_DIR = os.path.abspath(join(OUTPUT_DIR, \"images/\"))\n",
    "FEATURES_DIR = os.path.abspath(join(OUTPUT_DIR, \"features/\"))\n",
    "\n",
    "NULL_PROMPT = \"\"\n",
    "PROMPT = [\n",
    "    \"Bedroom interior, mid century modern, retro, vintage, designer furniture made of wood and plastic, concrete nightstand, wood walls, potted plant on a shelf, large window, outdoor cityscape view, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Dining room interior, mid century modern, retro, vintage, designer dining set made of wood and plastic, concrete floor, wood paneling, flower vase on a sideboard, large window overlooking a garden, outdoor countryside landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Office interior, mid century modern, retro, vintage, designer desk made of wood and plastic, concrete bookshelf, wood-paneled walls, potted plant on a window sill, large window with a view of the ocean, outdoor coastal landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Kitchen interior, mid century modern, retro, vintage, designer kitchen island made of wood and plastic, concrete countertops, wood cabinets, flowerpot on a windowsill, large window overlooking a lush garden, outdoor tropical landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Study room interior, mid century modern, retro, vintage, designer study desk made of wood and plastic, concrete floor, wood-paneled walls, potted bonsai tree on a shelf, large window with a view of the mountains, outdoor snowy landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Bathroom interior, mid century modern, retro, vintage, designer bathtub and sink made of wood and plastic, concrete walls, potted fern on a ledge, large window overlooking a tranquil lake, outdoor lakeside landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Studio interior, mid century modern, retro, vintage, designer studio set-up made of wood and plastic, concrete flooring, wood accent walls, flowerpot on a table, large window with a view of a futuristic cityscape, outdoor urban landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Lounge interior, mid century modern, retro, vintage, designer lounge chairs made of wood and plastic, concrete coffee table, wood-paneled walls, potted cactus on a shelf, large window overlooking a desert landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\"\n",
    "]\n",
    "\n",
    "# DEVICE = input(\"Set device: 'cuda:i' or 'cpu'\")\n",
    "DEVICE = None\n",
    "DEVICE = get_device(DEVICE)\n",
    "\n",
    "\n",
    "print(EMBEDDED_PROMPTS_DIR)\n",
    "print(OUTPUT_DIR)\n",
    "print(IMAGES_DIR)\n",
    "print(FEATURES_DIR)\n",
    "pt = ModelsPathTree(base_directory=base_dir)\n",
    "\n",
    "os.makedirs(EMBEDDED_PROMPTS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Dd_21FmhHA4B"
   },
   "outputs": [],
   "source": [
    "def calculate_sha256(tensor):\n",
    "    if tensor.device == \"cpu\":\n",
    "        tensor_bytes = tensor.numpy().tobytes()  # Convert tensor to a byte array\n",
    "    else:\n",
    "        tensor_bytes = tensor.cpu().numpy().tobytes()  # Convert tensor to a byte array\n",
    "    sha256_hash = hashlib.sha256(tensor_bytes)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "\n",
    "def embed_and_save_prompts(prompts: list, null_prompt=NULL_PROMPT):\n",
    "    null_prompt = null_prompt\n",
    "    prompts = prompts\n",
    "\n",
    "    clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
    "    clip_text_embedder.load_submodels(**pt.embedder_submodels)\n",
    "\n",
    "    null_cond = clip_text_embedder(null_prompt)\n",
    "    torch.save(null_cond, join(EMBEDDED_PROMPTS_DIR, \"null_cond.pt\"))\n",
    "    print(\n",
    "        \"Null prompt embedding saved at: \",\n",
    "        f\"{join(EMBEDDED_PROMPTS_DIR, 'null_cond.pt')}\",\n",
    "    )\n",
    "\n",
    "    embedded_prompts = clip_text_embedder(prompts)\n",
    "    torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_prompts.pt\"))\n",
    "\n",
    "    print(\n",
    "        \"Prompts embeddings saved at: \",\n",
    "        f\"{join(EMBEDDED_PROMPTS_DIR, 'embedded_prompts.pt')}\",\n",
    "    )\n",
    "\n",
    "    get_memory_status()\n",
    "    clip_text_embedder.to(\"cpu\")\n",
    "    del clip_text_embedder\n",
    "    torch.cuda.empty_cache()\n",
    "    get_memory_status()\n",
    "\n",
    "    return embedded_prompts, null_cond\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    import numpy as np\n",
    "\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzSdG4VCHCS5",
    "outputId": "b6e75246-492f-4f61-dd91-5918c239ef41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `device` is None. Using device  cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
      "INFO: Device given. Using device cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
      "Null prompt embedding saved at:  /root/repo-personal/kcg-ml-sd1p4/input/embedded_prompts/null_cond.pt\n",
      "Prompts embeddings saved at:  /root/repo-personal/kcg-ml-sd1p4/input/embedded_prompts/embedded_prompts.pt\n",
      "Total: 24259 MiB\n",
      "Free: 14544 MiB\n",
      "Used: 9715 MiB\n",
      "Total: 24259 MiB\n",
      "Free: 15700 MiB\n",
      "Used: 8559 MiB\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each prompt\n",
    "embedded_prompts, null_prompt = embed_and_save_prompts(PROMPT)\n",
    "embedding = embedded_prompts\n",
    "num_images = embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "XJmQRUQaO_gE"
   },
   "outputs": [],
   "source": [
    "# generate noise and add to the embedded prompt\n",
    "def add_noise_to_embeds(embedded_prompts, noise_multiplier):\n",
    "    embedded_prompts.mean(dim=2), embedded_prompts.std(dim=2)\n",
    "    noise = torch.normal(mean=embedded_prompts.mean(dim=2), std=embedded_prompts.std(dim=2))\n",
    "    dist = torch.distributions.normal.Normal(\n",
    "        loc=embedded_prompts.mean(dim=2), scale=embedded_prompts.std(dim=2)\n",
    "    )\n",
    "    noise = dist.sample(sample_shape=torch.Size([768])).permute(1, 0, 2).permute(0, 2, 1)\n",
    "    noise.shape\n",
    "\n",
    "    generator = torch.Generator(device=DEVICE).manual_seed(SEED) if SEED is not None else torch.Generator(device=DEVICE)\n",
    "\n",
    "    dist = torch.distributions.normal.Normal(\n",
    "        loc=embedded_prompts.mean(dim=2), scale=embedded_prompts.std(dim=2)\n",
    "    )\n",
    "    noise = dist.sample(sample_shape=torch.Size([768])).permute(1, 0, 2).permute(0, 2, 1)\n",
    "\n",
    "    embedding_e = embedded_prompts + noise_multiplier * noise\n",
    "\n",
    "    return embedding_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYH6CWsOMbR-",
    "outputId": "51cd7e1b-27b7-4cc4-9652-0406b46e78bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
      "WARNING: `LatentDiffusion` model is `None` given. Initialize one with the appropriate method.\n",
      "INFO: Device given. Using device cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
      "Autoencoder loaded from: /root/repo-personal/kcg-ml-sd1p4/input/model/autoencoder/autoencoder.ckpt\n",
      "Decoder loaded from: /root/repo-personal/kcg-ml-sd1p4/input/model/autoencoder/decoder.ckpt\n",
      "INFO: `device` is None. Using device  cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n"
     ]
    }
   ],
   "source": [
    "# Load Stable Diffusion\n",
    "sd = StableDiffusion(device=DEVICE, n_steps=N_STEPS)\n",
    "sd.quick_initialize().load_autoencoder(**pt.autoencoder).load_decoder(**pt.decoder)\n",
    "sd.model.load_unet(**pt.unet)\n",
    "\n",
    "# Load chadscore and clip\n",
    "import clip\n",
    "\n",
    "# Test calculate chadscore\n",
    "chad = ChadPredictor(768)\n",
    "chad.load(\"./input/model/aesthetic_scorer/sac+logos+ava1-l14-linearMSE.pth\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "  chad.model.to(\"cuda\")\n",
    "chad.model.eval()\n",
    "image_features_clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)  #RN50x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "kqPwnmEYeXod"
   },
   "outputs": [],
   "source": [
    "def generate_images_from_embeddings(embedded_prompts_array, null_prompt):\n",
    "  # 'embedded_prompts_array' is the array of shape (i, 77, 768)\n",
    "  image = sd.generate_images_from_embeddings(\n",
    "      seed=SEED, embedded_prompt=embedded_prompts_array[i:i+1], null_prompt=null_prompt\n",
    "  )\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "3VxFRyYQK_oq",
    "outputId": "ff985444-3324-42ec-e034-fcc8ca7bafe5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">\n",
       "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,154.32ms</span>\n",
       "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,176.93ms</span>\n",
       "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,149.66ms</span>\n",
       "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,169.34ms</span>\n",
       "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,174.42ms</span>\n",
       "      Sample<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,174.08ms</span>\n",
       "      Sample<span style=\"color: #D160C4\">   50%</span><span style=\"color: #208FFB\">\t1,195.48ms</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pygad\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to calculate the chad score for batch of images\n",
    "def calculate_chad_score(ga_instance, solution, solution_idx):\n",
    "  # Convert the solution back to the original shape (1, 77, 768)\n",
    "  solution_reshaped = solution.reshape(1, 77, 768)\n",
    "\n",
    "  # Convert the numpy array to a PyTorch tensor\n",
    "  solution_reshaped = torch.tensor(solution_reshaped, dtype=torch.float32)\n",
    "\n",
    "  # Copy the tensor to CUDA device if 'device' is 'cuda'\n",
    "  if device == 'cuda':\n",
    "    solution_reshaped = solution_reshaped.to(device)\n",
    "\n",
    "  # Generate an image using the solution\n",
    "  image = generate_images_from_embeddings(solution_reshaped, null_prompt)\n",
    "\n",
    "  pil_image = to_pil(image[0])  # Convert to (height, width, channels)\n",
    "  unsqueezed_image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    image_features = image_features_clip_model.encode_image(unsqueezed_image)\n",
    "\n",
    "    im_emb_arr = normalized(image_features.cpu().detach().numpy() )\n",
    "    prediction = chad(torch.from_numpy(im_emb_arr).to(device).type(torch.cuda.FloatTensor))\n",
    "    chad_score = prediction.item()\n",
    "  return chad_score\n",
    "\n",
    "# Define the GA loop function\n",
    "def genetic_algorithm_loop(sd, embedded_prompts, null_prompt, generations=10, population_size=POPULATION_SIZE, mutation_rate=0.4, num_parents_mating=2):\n",
    "    # Move the 'embedded_prompts' tensor to CPU memory\n",
    "    embedded_prompts_cpu = embedded_prompts.cpu()\n",
    "\n",
    "    # Reshape the 'embedded_prompts' tensor to a 2D numpy array\n",
    "    embedded_prompts_array = embedded_prompts_cpu.detach().numpy()\n",
    "    num_individuals = embedded_prompts_array.shape[0]\n",
    "    num_genes = embedded_prompts_array.shape[1] * embedded_prompts_array.shape[2]\n",
    "    embedded_prompts_list = embedded_prompts_array.reshape(num_individuals, num_genes).tolist()\n",
    "\n",
    "    # Initialize the GA\n",
    "    ga_instance = pygad.GA(num_generations=generations,\n",
    "                           num_parents_mating=num_parents_mating,\n",
    "                           fitness_func=calculate_chad_score,\n",
    "                           sol_per_pop=population_size,\n",
    "                           num_genes=num_genes,\n",
    "                           initial_population=embedded_prompts_list,\n",
    "                           mutation_percent_genes=mutation_rate*100)\n",
    "\n",
    "    # Run the GA loop\n",
    "    for generation in range(ga_instance.num_generations):\n",
    "\n",
    "        # Mutate each individual in the population with noise\n",
    "        #for i in range(ga_instance.population.shape[0]):\n",
    "           # Extract the individual's solution from the population\n",
    "        #    solution = ga_instance.population[i]\n",
    "\n",
    "            # Add noise to the solution using the 'add_noise_to_embeds' function\n",
    "            #noisy_solution = add_noise_to_embeds(solution, random.uniform(0, NOISE_MAX_MULTIPLIER))\n",
    "            #ga_instance.population[i] = noisy_solution.reshape(-1)\n",
    "\n",
    "        # Get the best solution from the GA\n",
    "        best_solution, best_solution_fitness, best_solution_idx = ga_instance.best_solution()\n",
    "\n",
    "        print(\"Generation:\", generation)\n",
    "        print(\"The current best chadscore for our youngsters is:\", best_solution_fitness)\n",
    "\n",
    "        # Save images every N generations (ex. 10)\n",
    "        if generation % 1 == 0:\n",
    "          # Convert the solution back to the original shape (1, 77, 768)\n",
    "          solution_reshaped = best_solution.reshape(1, 77, 768)\n",
    "          solution_reshaped = torch.tensor(solution_reshaped, dtype=torch.float32)\n",
    "\n",
    "          # Copy the tensor to CUDA device if 'device' is 'cuda'\n",
    "          if device == 'cuda':\n",
    "            solution_reshaped = solution_reshaped.to(device)\n",
    "\n",
    "          image = generate_images_from_embeddings(solution_reshaped, null_prompt)\n",
    "          pil_image = to_pil(image[0])\n",
    "          filename=f\"{IMAGES_DIR}/{generation}.png\"\n",
    "          pil_image.show()\n",
    "          pil_image.save(filename)\n",
    "\n",
    "    # Get the final best solution and images\n",
    "    best_solution, best_solution_fitness = ga_instance.best_solution()\n",
    "    best_solution = best_solution.reshape(1, 77, 768)  # Reshape the best solution to the correct shape\n",
    "    final_image = sd.generate_images_from_embeddings(embedded_prompt=best_solution, null_prompt=null_prompt)\n",
    "\n",
    "    # Save the final best solution images\n",
    "    final_filename=f\"{IMAGES_DIR}/final.png\"\n",
    "    pil_final = to_pil(final_image[0])\n",
    "    pil_final.show()\n",
    "    pil_final.save(final_filename)\n",
    "\n",
    "    return best_solution\n",
    "\n",
    "# Call the GA loop function with your initialized StableDiffusion model\n",
    "best_solution = genetic_algorithm_loop(sd, embedded_prompts, null_prompt)\n",
    "\n",
    "print(\"Best solution found!\")\n",
    "\n",
    "torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_final_solution.pt\"))\n",
    "print(\"Saving solution...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UL-xzoZT8qvW"
   },
   "outputs": [],
   "source": [
    "# Clean unused loaded models\n",
    "del preprocess, image_features_clip_model, sd"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
