{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKD3sXj6E72o"
      },
      "outputs": [],
      "source": [
        "!pip3 install -r requirements.txt --ignore-installed"
      ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `device` is None. Using device  cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3090.\n",
      "/root/repo-personal/kcg-ml-sd1p4/input/embedded_prompts\n",
      "/root/repo-personal/kcg-ml-sd1p4/output/ga\n",
      "/root/repo-personal/kcg-ml-sd1p4/output/ga/images\n",
      "/root/repo-personal/kcg-ml-sd1p4/output/ga/features\n"
     ]
    }
   ],
   "source": [
    "from chad_score.chad_score_predictor import ChadPredictor\n",
    "import torch\n",
    "from typing import List\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "\n",
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder\n",
    "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
    "from stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.constants import IODirectoryTree\n",
    "from stable_diffusion.utils_backend import get_device, get_memory_status,get_memory_status\n",
    "from stable_diffusion.utils_image import to_pil,save_image_grid, show_image_grid\n",
    "from os.path import join\n",
    "import os, sys\n",
    "\n",
    "base_dir = \"./\"\n",
    "sys.path.insert(0, base_dir)\n",
    "\n",
    "# Variables\n",
    "SEED = 1337\n",
    "NOISE_MAX_MULTIPLIER = 0.5\n",
    "BATCH_SIZE = 1\n",
    "POPULATION_SIZE = 12\n",
    "GEN_IMAGE_N_GENERATIONS = 50\n",
    "N_STEPS = 10\n",
    "EMBEDDED_PROMPTS_DIR = os.path.abspath(join(base_dir, \"./input/embedded_prompts/\"))\n",
    "OUTPUT_DIR = os.path.abspath(\n",
    "    os.path.join(base_dir, \"./output/ga/\")\n",
    ")\n",
    "IMAGES_DIR = os.path.abspath(join(OUTPUT_DIR, \"images/\"))\n",
    "FEATURES_DIR = os.path.abspath(join(OUTPUT_DIR, \"features/\"))\n",
    "\n",
    "NULL_PROMPT = \"\"\n",
    "PROMPT = [\n",
    "    \"Bedroom interior, mid century modern, retro, vintage, designer furniture made of wood and plastic, concrete nightstand, wood walls, potted plant on a shelf, large window, outdoor cityscape view, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Dining room interior, mid century modern, retro, vintage, designer dining set made of wood and plastic, concrete floor, wood paneling, flower vase on a sideboard, large window overlooking a garden, outdoor countryside landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Office interior, mid century modern, retro, vintage, designer desk made of wood and plastic, concrete bookshelf, wood-paneled walls, potted plant on a window sill, large window with a view of the ocean, outdoor coastal landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Kitchen interior, mid century modern, retro, vintage, designer kitchen island made of wood and plastic, concrete countertops, wood cabinets, flowerpot on a windowsill, large window overlooking a lush garden, outdoor tropical landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Study room interior, mid century modern, retro, vintage, designer study desk made of wood and plastic, concrete floor, wood-paneled walls, potted bonsai tree on a shelf, large window with a view of the mountains, outdoor snowy landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Bathroom interior, mid century modern, retro, vintage, designer bathtub and sink made of wood and plastic, concrete walls, potted fern on a ledge, large window overlooking a tranquil lake, outdoor lakeside landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Studio interior, mid century modern, retro, vintage, designer studio set-up made of wood and plastic, concrete flooring, wood accent walls, flowerpot on a table, large window with a view of a futuristic cityscape, outdoor urban landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\",\n",
    "    \"Lounge interior, mid century modern, retro, vintage, designer lounge chairs made of wood and plastic, concrete coffee table, wood-paneled walls, potted cactus on a shelf, large window overlooking a desert landscape, beautiful sunset, cinematic, concept art, sustainable architecture, octane render, utopia, ethereal, cinematic light.\"\n",
    "]\n",
    "\n",
    "# DEVICE = input(\"Set device: 'cuda:i' or 'cpu'\")\n",
    "DEVICE = None\n",
    "DEVICE = get_device(DEVICE)\n",
    "\n",
    "\n",
    "print(EMBEDDED_PROMPTS_DIR)\n",
    "print(OUTPUT_DIR)\n",
    "print(IMAGES_DIR)\n",
    "print(FEATURES_DIR)\n",
    "pt = IODirectoryTree(base_directory=base_dir)\n",
    "\n",
    "os.makedirs(EMBEDDED_PROMPTS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)"
   ]
  }
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Dd_21FmhHA4B"
   },
   "outputs": [],
   "source": [
    "def calculate_sha256(tensor):\n",
    "    if tensor.device == \"cpu\":\n",
    "        tensor_bytes = tensor.numpy().tobytes()  # Convert tensor to a byte array\n",
    "    else:\n",
    "        tensor_bytes = tensor.cpu().numpy().tobytes()  # Convert tensor to a byte array\n",
    "    sha256_hash = hashlib.sha256(tensor_bytes)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "\n",
    "def embed_and_save_prompts(prompts: list, null_prompt=NULL_PROMPT):\n",
    "    null_prompt = null_prompt\n",
    "    prompts = prompts\n",
    "\n",
    "    clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
    "    clip_text_embedder.load_submodels(**pt.embedder_submodels)\n",
    "\n",
    "    null_cond = clip_text_embedder(null_prompt)\n",
    "    torch.save(null_cond, join(EMBEDDED_PROMPTS_DIR, \"null_cond.pt\"))\n",
    "    print(\n",
    "        \"Null prompt embedding saved at: \",\n",
    "        f\"{join(EMBEDDED_PROMPTS_DIR, 'null_cond.pt')}\",\n",
    "    )\n",
    "\n",
    "    embedded_prompts = clip_text_embedder(prompts)\n",
    "    torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_prompts.pt\"))\n",
    "\n",
    "    print(\n",
    "        \"Prompts embeddings saved at: \",\n",
    "        f\"{join(EMBEDDED_PROMPTS_DIR, 'embedded_prompts.pt')}\",\n",
    "    )\n",
    "\n",
    "    get_memory_status()\n",
    "    clip_text_embedder.to(\"cpu\")\n",
    "    del clip_text_embedder\n",
    "    torch.cuda.empty_cache()\n",
    "    get_memory_status()\n",
    "\n",
    "    return embedded_prompts, null_cond\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    import numpy as np\n",
    "\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 scripts/process_base_model.py ./input/model/v1-5-pruned-emaonly.safetensors\n",
        "!python3 scripts/setup_models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxHm8JtTTcFA",
        "outputId": "72d19199-710f-48fc-cbc2-60c7c7691513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7ff3d8066130>: NVIDIA GeForce RTX 3090\u001b[0m\r\n",
            "\r                                                                                                    \rautoencoder initialization...\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7ff3d8066130>: NVIDIA GeForce RTX 3090\u001b[0m\r\n",
            "\r\r\n",
            "  encoder initialization...\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7ff3d8066130>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "  encoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t1,385.96ms\u001b[0m\n",
            "  decoder initialization...\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7ff3d8066130>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "  decoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t735.81ms\u001b[0m\n",
            "\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7ff3d8066130>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "autoencoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t2,132.96ms\u001b[0m\n",
            "CLIP Embedder initialization...\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7ff3d8066130>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7ff3d8066130>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "Tokenizer successfully loaded from : ./io/input/model/clip/text_embedder/tokenizer\n",
            "\n",
            "CLIP Embedder initialization\u001b[31m...[FAIL]\u001b[0m\u001b[34m\t131.87ms\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py\", line 629, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\", line 417, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './io/input/model/clip/text_embedder/clip-vit-large-patch14'. Use `repo_type` argument if needed.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"scripts/process_base_model.py\", line 87, in <module>\n",
            "    model = initialize_latent_diffusion(\n",
            "  File \"/devbox/kcg-ml-sd1p4/./stable_diffusion/utils_model.py\", line 229, in initialize_latent_diffusion\n",
            "    clip_text_embedder = initialize_clip_embedder(device=device, force_submodels_init=force_submodels_init)\n",
            "  File \"/devbox/kcg-ml-sd1p4/./stable_diffusion/utils_model.py\", line 168, in initialize_clip_embedder\n",
            "    clip_text_embedder.load_submodels()\n",
            "  File \"/devbox/kcg-ml-sd1p4/./stable_diffusion/model/clip_text_embedder/clip_text_embedder.py\", line 65, in load_submodels\n",
            "    self.text_model = CLIPTextModel.from_pretrained(text_model_path, local_files_only=True,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 2251, in from_pretrained\n",
            "    config, model_kwargs = cls.config_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/clip/configuration_clip.py\", line 130, in from_pretrained\n",
            "    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py\", line 574, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py\", line 650, in _get_config_dict\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load the configuration of './io/input/model/clip/text_embedder/clip-vit-large-patch14'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './io/input/model/clip/text_embedder/clip-vit-large-patch14' is the correct path to a directory containing a config.json file\n",
            "USE_CKPT False\n",
            "\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f61e4e13870>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "autoencoder initialization...\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f61e4e13870>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "\n",
            "  encoder initialization...\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f61e4e13870>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "  encoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t1,381.14ms\u001b[0m\n",
            "  decoder initialization...\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f61e4e13870>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "  decoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t749.14ms\u001b[0m\n",
            "\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f61e4e13870>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "autoencoder initialization\u001b[32m...[DONE]\u001b[0m\u001b[34m\t2,140.22ms\u001b[0m\n",
            "CLIP Embedder initialization...\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f61e4e13870>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f61e4e13870>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "Tokenizer successfully loaded from : ./io/input/model/clip/text_embedder/tokenizer\n",
            "\n",
            "CLIP Embedder initialization\u001b[31m...[FAIL]\u001b[0m\u001b[34m\t131.47ms\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py\", line 629, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\", line 417, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\", line 110, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\", line 158, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './io/input/model/clip/text_embedder/clip-vit-large-patch14'. Use `repo_type` argument if needed.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"scripts/setup_models.py\", line 132, in <module>\n",
            "    model = initialize_latent_diffusion(\n",
            "  File \"/devbox/kcg-ml-sd1p4/./stable_diffusion/utils_model.py\", line 229, in initialize_latent_diffusion\n",
            "    clip_text_embedder = initialize_clip_embedder(device=device, force_submodels_init=force_submodels_init)\n",
            "  File \"/devbox/kcg-ml-sd1p4/./stable_diffusion/utils_model.py\", line 168, in initialize_clip_embedder\n",
            "    clip_text_embedder.load_submodels()\n",
            "  File \"/devbox/kcg-ml-sd1p4/./stable_diffusion/model/clip_text_embedder/clip_text_embedder.py\", line 65, in load_submodels\n",
            "    self.text_model = CLIPTextModel.from_pretrained(text_model_path, local_files_only=True,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 2251, in from_pretrained\n",
            "    config, model_kwargs = cls.config_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/clip/configuration_clip.py\", line 130, in from_pretrained\n",
            "    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py\", line 574, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py\", line 650, in _get_config_dict\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load the configuration of './io/input/model/clip/text_embedder/clip-vit-large-patch14'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './io/input/model/clip/text_embedder/clip-vit-large-patch14' is the correct path to a directory containing a config.json file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to generate prompts\n",
        "def generate_prompts(prompt_segments, num_prompts=6):\n",
        "    # Select 6 random segments from the prompt_segments list\n",
        "    selected_prompts = random.sample(prompt_segments, num_prompts)\n",
        "\n",
        "    # Add modifiers to the selected prompts\n",
        "    modifiers = [\n",
        "        'beautiful', 'gorgeous', 'stunning', 'charming', 'captivating', 'breathtaking',\n",
        "        'masterpiece', 'exquisite', 'magnificent', 'majestic', 'elegant', 'sublime',\n",
        "        'ugly', 'hideous', 'grotesque', 'repulsive', 'disgusting', 'revolting',\n",
        "        'futuristic', 'cyberpunk', 'hi-tech', 'advanced', 'innovative', 'modern',\n",
        "        'fantasy', 'mythical', 'scifi', 'side scrolling', 'character', 'side scrolling',\n",
        "        'white background', 'centered', 'full character', 'no background', 'not centered',\n",
        "        'line drawing', 'sketch', 'black and white', 'colored', 'offset', 'video game']\n",
        "    prompts_with_modifiers = [f\"{modifier} {prompt}\" for modifier in modifiers for prompt in selected_prompts]\n",
        "\n",
        "    # Join the prompts with commas to separate phases\n",
        "    prompt_phrases = \", \".join(prompts_with_modifiers)\n",
        "\n",
        "    return prompt_phrases\n",
        "\n",
        "# List of prompt segments\n",
        "prompt_segments = ['chibi', 'waifu', 'cyborg', 'dragon', 'android', 'nekomimi', 'mecha', 'kitsune', 'AI companion', 'furry detective', 'robot butler', 'futuristic steampunk', 'cybernetic implants',\n",
        "                   'anthropomorphic AI', 'mechanical wizard', 'kemonomimi', 'android rebellion', 'magical robot pet', 'intergalactic furball', 'cyberpunk android', 'shapeshifting furry',\n",
        "                   'mech pilot', 'furry time traveler',\n",
        "]\n",
        "\n",
        "\n",
        "# Generate 6 random prompts with modifiers (initial population)\n",
        "PROMPT = generate_prompts(prompt_segments)\n",
        "\n",
        "# Print the generated prompts\n",
        "print(PROMPT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqjrIYWnJhUz",
        "outputId": "7f334315-e38a-4ad5-9dea-d3d7885a298e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beautiful mecha, beautiful furry detective, beautiful android, beautiful dragon, beautiful intergalactic furball, beautiful mechanical wizard, gorgeous mecha, gorgeous furry detective, gorgeous android, gorgeous dragon, gorgeous intergalactic furball, gorgeous mechanical wizard, stunning mecha, stunning furry detective, stunning android, stunning dragon, stunning intergalactic furball, stunning mechanical wizard, charming mecha, charming furry detective, charming android, charming dragon, charming intergalactic furball, charming mechanical wizard, captivating mecha, captivating furry detective, captivating android, captivating dragon, captivating intergalactic furball, captivating mechanical wizard, breathtaking mecha, breathtaking furry detective, breathtaking android, breathtaking dragon, breathtaking intergalactic furball, breathtaking mechanical wizard, masterpiece mecha, masterpiece furry detective, masterpiece android, masterpiece dragon, masterpiece intergalactic furball, masterpiece mechanical wizard, exquisite mecha, exquisite furry detective, exquisite android, exquisite dragon, exquisite intergalactic furball, exquisite mechanical wizard, magnificent mecha, magnificent furry detective, magnificent android, magnificent dragon, magnificent intergalactic furball, magnificent mechanical wizard, majestic mecha, majestic furry detective, majestic android, majestic dragon, majestic intergalactic furball, majestic mechanical wizard, elegant mecha, elegant furry detective, elegant android, elegant dragon, elegant intergalactic furball, elegant mechanical wizard, sublime mecha, sublime furry detective, sublime android, sublime dragon, sublime intergalactic furball, sublime mechanical wizard, ugly mecha, ugly furry detective, ugly android, ugly dragon, ugly intergalactic furball, ugly mechanical wizard, hideous mecha, hideous furry detective, hideous android, hideous dragon, hideous intergalactic furball, hideous mechanical wizard, grotesque mecha, grotesque furry detective, grotesque android, grotesque dragon, grotesque intergalactic furball, grotesque mechanical wizard, repulsive mecha, repulsive furry detective, repulsive android, repulsive dragon, repulsive intergalactic furball, repulsive mechanical wizard, disgusting mecha, disgusting furry detective, disgusting android, disgusting dragon, disgusting intergalactic furball, disgusting mechanical wizard, revolting mecha, revolting furry detective, revolting android, revolting dragon, revolting intergalactic furball, revolting mechanical wizard, futuristic mecha, futuristic furry detective, futuristic android, futuristic dragon, futuristic intergalactic furball, futuristic mechanical wizard, cyberpunk mecha, cyberpunk furry detective, cyberpunk android, cyberpunk dragon, cyberpunk intergalactic furball, cyberpunk mechanical wizard, hi-tech mecha, hi-tech furry detective, hi-tech android, hi-tech dragon, hi-tech intergalactic furball, hi-tech mechanical wizard, advanced mecha, advanced furry detective, advanced android, advanced dragon, advanced intergalactic furball, advanced mechanical wizard, innovative mecha, innovative furry detective, innovative android, innovative dragon, innovative intergalactic furball, innovative mechanical wizard, modern mecha, modern furry detective, modern android, modern dragon, modern intergalactic furball, modern mechanical wizard, fantasy mecha, fantasy furry detective, fantasy android, fantasy dragon, fantasy intergalactic furball, fantasy mechanical wizard, mythical mecha, mythical furry detective, mythical android, mythical dragon, mythical intergalactic furball, mythical mechanical wizard, scifi mecha, scifi furry detective, scifi android, scifi dragon, scifi intergalactic furball, scifi mechanical wizard, side scrolling mecha, side scrolling furry detective, side scrolling android, side scrolling dragon, side scrolling intergalactic furball, side scrolling mechanical wizard, character mecha, character furry detective, character android, character dragon, character intergalactic furball, character mechanical wizard, side scrolling mecha, side scrolling furry detective, side scrolling android, side scrolling dragon, side scrolling intergalactic furball, side scrolling mechanical wizard, white background mecha, white background furry detective, white background android, white background dragon, white background intergalactic furball, white background mechanical wizard, centered mecha, centered furry detective, centered android, centered dragon, centered intergalactic furball, centered mechanical wizard, full character mecha, full character furry detective, full character android, full character dragon, full character intergalactic furball, full character mechanical wizard, no background mecha, no background furry detective, no background android, no background dragon, no background intergalactic furball, no background mechanical wizard, not centered mecha, not centered furry detective, not centered android, not centered dragon, not centered intergalactic furball, not centered mechanical wizard, line drawing mecha, line drawing furry detective, line drawing android, line drawing dragon, line drawing intergalactic furball, line drawing mechanical wizard, sketch mecha, sketch furry detective, sketch android, sketch dragon, sketch intergalactic furball, sketch mechanical wizard, black and white mecha, black and white furry detective, black and white android, black and white dragon, black and white intergalactic furball, black and white mechanical wizard, colored mecha, colored furry detective, colored android, colored dragon, colored intergalactic furball, colored mechanical wizard, offset mecha, offset furry detective, offset android, offset dragon, offset intergalactic furball, offset mechanical wizard, video game mecha, video game furry detective, video game android, video game dragon, video game intergalactic furball, video game mechanical wizard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd_21FmhHA4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "8ccc1d13-860f-490c-dbb7-9e629428c069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f54cccb33f0>: NVIDIA GeForce RTX 3090\u001b[0m\n",
            "\u001b[1;32mINFO: Using CUDA device <built-in method index of str object at 0x7f54cccb33f0>: NVIDIA GeForce RTX 3090\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(l2, axis)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Generate embeddings for each prompt\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m embedded_prompts, null_prompt \u001b[38;5;241m=\u001b[39m \u001b[43membed_and_save_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m embedding \u001b[38;5;241m=\u001b[39m embedded_prompts\n\u001b[1;32m     34\u001b[0m num_images \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
            "Cell \u001b[0;32mIn[43], line 6\u001b[0m, in \u001b[0;36membed_and_save_prompts\u001b[0;34m(prompts, null_prompt)\u001b[0m\n\u001b[1;32m      3\u001b[0m prompts \u001b[38;5;241m=\u001b[39m prompts\n\u001b[1;32m      5\u001b[0m clip_text_embedder \u001b[38;5;241m=\u001b[39m CLIPTextEmbedder(device\u001b[38;5;241m=\u001b[39mget_device())\n\u001b[0;32m----> 6\u001b[0m clip_text_embedder\u001b[38;5;241m.\u001b[39mload_submodels(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedder_submodels\u001b[49m)\n\u001b[1;32m      8\u001b[0m embedded_prompts \u001b[38;5;241m=\u001b[39m clip_text_embedder(prompts)\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedded_prompts.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[0;32m/devbox/kcg-ml-sd1p4/stable_diffusion/constants.py:333\u001b[0m, in \u001b[0;36mIODirectoryTree.embedder_submodels\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membedder_submodels\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_path,\n\u001b[0;32m--> 333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_path\u001b[49m,\n\u001b[1;32m    334\u001b[0m     }\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'IODirectoryTree' object has no attribute 'transformer_path'"
          ]
        }
      ],
      "source": [
        "def embed_and_save_prompts(prompts: list, null_prompt=NULL_PROMPT):\n",
        "    null_prompt = null_prompt\n",
        "    prompts = prompts\n",
        "\n",
        "    clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
        "    clip_text_embedder.load_submodels(**pt.embedder_submodels)\n",
        "\n",
        "    embedded_prompts = clip_text_embedder(prompts)\n",
        "    torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_prompts.pt\"))\n",
        "\n",
        "    print(\n",
        "        \"Prompts embeddings saved at: \",\n",
        "        f\"{join(EMBEDDED_PROMPTS_DIR, 'embedded_prompts.pt')}\",\n",
        "    )\n",
        "\n",
        "    get_memory_status()\n",
        "    clip_text_embedder.to(\"cpu\")\n",
        "    del clip_text_embedder\n",
        "    torch.cuda.empty_cache()\n",
        "    get_memory_status()\n",
        "\n",
        "    return embedded_prompts, null_cond\n",
        "\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    import numpy as np\n",
        "\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2 == 0] = 1\n",
        "    return a / np.expand_dims(l2, axis)\n",
        "\n",
        "# Generate embeddings for each prompt\n",
        "embedded_prompts, null_prompt = embed_and_save_prompts(PROMPT)\n",
        "embedding = embedded_prompts\n",
        "num_images = embedding.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJmQRUQaO_gE"
      },
      "outputs": [],
      "source": [
        "# Mutation function to add noise to numpy arrays\n",
        "def mutate(embedded_prompts):\n",
        "    # Calculate the mean and standard deviation along the last axis (axis=2)\n",
        "    mean_embedded_prompts = np.mean(embedded_prompts, axis=2)\n",
        "    std_embedded_prompts = np.std(embedded_prompts, axis=2)\n",
        "\n",
        "    # Generate noise with the same shape as the embedded_prompts\n",
        "    noise_shape = (embedded_prompts.shape[0], embedded_prompts.shape[1], 768)\n",
        "    noise = np.random.normal(loc=mean_embedded_prompts, scale=std_embedded_prompts, size=noise_shape)\n",
        "\n",
        "    # Generate a random value for the noise multiplier\n",
        "    random_val = np.random.rand()\n",
        "    noise_multiplier = min_val + random_val * (0.1 - 0.01)\n",
        "\n",
        "    # Add noise to the embedded_prompts\n",
        "    embedding_e = embedded_prompts + noise_multiplier * noise\n",
        "\n",
        "    return embedding_e\n",
        "\n",
        "# Crossover function (using single-point crossover)\n",
        "def crossover(individual1, individual2):\n",
        "    # Check if the individuals have the same shape\n",
        "    if individual1.shape != individual2.shape:\n",
        "        raise ValueError(\"Individuals must have the same shape for crossover.\")\n",
        "\n",
        "    # Flatten the individuals to make it easier to perform crossover\n",
        "    flat_individual1 = individual1.flatten()\n",
        "    flat_individual2 = individual2.flatten()\n",
        "\n",
        "    # Randomly choose the crossover point\n",
        "    crossover_point = np.random.randint(0, flat_individual1.shape[0])\n",
        "\n",
        "    # Perform crossover by swapping genetic material\n",
        "    offspring1 = np.concatenate((flat_individual1[:crossover_point], flat_individual2[crossover_point:]))\n",
        "    offspring2 = np.concatenate((flat_individual2[:crossover_point], flat_individual1[crossover_point:]))\n",
        "\n",
        "    # Reshape the offspring back to the original shape\n",
        "    offspring1 = offspring1.reshape(individual1.shape)\n",
        "    offspring2 = offspring2.reshape(individual2.shape)\n",
        "\n",
        "    return offspring1, offspring2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYH6CWsOMbR-"
      },
      "outputs": [],
      "source": [
        "# Load Stable Diffusion\n",
        "sd = StableDiffusion(device=DEVICE, n_steps=N_STEPS)\n",
        "sd.quick_initialize().load_autoencoder(**pt.autoencoder).load_decoder(**pt.decoder)\n",
        "sd.model.load_unet(**pt.unet)\n",
        "\n",
        "# Load chadscore and clip\n",
        "import clip\n",
        "\n",
        "# Test calculate chadscore\n",
        "chad = ChadPredictor(768)\n",
        "chad.load(\"./input/model/aesthetic_scorer/sac+logos+ava1-l14-linearMSE.pth\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "  chad.model.to(\"cuda\")\n",
        "chad.model.eval()\n",
        "image_features_clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)  #RN50x64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqPwnmEYeXod"
      },
      "outputs": [],
      "source": [
        "def generate_images_from_embeddings(embedded_prompts_array, null_prompt):\n",
        "  # 'embedded_prompts_array' is the array of shape (i, 77, 768)\n",
        "  image = sd.generate_images_from_embeddings(\n",
        "      seed=SEED, embedded_prompt=embedded_prompts_array[i:i+1], null_prompt=null_prompt\n",
        "  )\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VxFRyYQK_oq"
      },
      "outputs": [],
      "source": [
        "import pygad\n",
        "\n",
        "# Function to calculate the chad score for batch of images\n",
        "def calculate_chad_score(ga_instance, solution, solution_idx):\n",
        "  # Convert the solution back to the original shape (1, 77, 768)\n",
        "  solution_reshaped = solution.reshape(1, 77, 768)\n",
        "\n",
        "  # Convert the numpy array to a PyTorch tensor\n",
        "  solution_reshaped = torch.tensor(solution_reshaped, dtype=torch.float32)\n",
        "\n",
        "  # Copy the tensor to CUDA device if 'device' is 'cuda'\n",
        "  if device == 'cuda':\n",
        "    solution_reshaped = solution_reshaped.to(device)\n",
        "\n",
        "  # Generate an image using the solution\n",
        "  image = generate_images_from_embeddings(solution_reshaped, null_prompt)\n",
        "\n",
        "  pil_image = to_pil(image[0])  # Convert to (height, width, channels)\n",
        "  unsqueezed_image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    image_features = image_features_clip_model.encode_image(unsqueezed_image)\n",
        "\n",
        "    im_emb_arr = normalized(image_features.cpu().detach().numpy() )\n",
        "    prediction = chad(torch.from_numpy(im_emb_arr).to(device).type(torch.cuda.FloatTensor))\n",
        "    chad_score = prediction.item()\n",
        "  return chad_score\n",
        "\n",
        "# Define the GA loop function\n",
        "def genetic_algorithm_loop(sd, embedded_prompts, null_prompt, generations=10, population_size=POPULATION_SIZE, mutation_rate=0.4, num_parents_mating=2):\n",
        "    # Move the 'embedded_prompts' tensor to CPU memory\n",
        "    embedded_prompts_cpu = embedded_prompts.cpu()\n",
        "\n",
        "    # Reshape the 'embedded_prompts' tensor to a 2D numpy array\n",
        "    embedded_prompts_array = embedded_prompts_cpu.detach().numpy()\n",
        "    num_individuals = embedded_prompts_array.shape[0]\n",
        "    num_genes = embedded_prompts_array.shape[1] * embedded_prompts_array.shape[2]\n",
        "    embedded_prompts_list = embedded_prompts_array.reshape(num_individuals, num_genes).tolist()\n",
        "\n",
        "    # Initialize the GA\n",
        "    ga_instance = pygad.GA(num_generations=generations,\n",
        "                           num_parents_mating=num_parents_mating,\n",
        "                           fitness_func=calculate_chad_score,\n",
        "                           sol_per_pop=population_size,\n",
        "                           num_genes=num_genes,\n",
        "                           initial_population=embedded_prompts_list,\n",
        "                           mutation_percent_genes=mutation_rate*100)\n",
        "\n",
        "    # Run the GA loop\n",
        "    for generation in range(ga_instance.num_generations):\n",
        "      # Perform crossover to create new individuals\n",
        "        parents = ga_instance.population.copy()\n",
        "        for parent1, parent2 in zip(parents[::2], parents[1::2]):\n",
        "            child1, child2 = crossover(parent1, parent2)\n",
        "            ga_instance.population.extend([child1, child2])\n",
        "\n",
        "        # Perform mutation on some individuals\n",
        "        for idx, individual in enumerate(ga_instance.population):\n",
        "            if np.random.rand() < ga_instance.mutation_percent:\n",
        "                ga_instance.population[idx] = mutate(individual, mutation_rate)\n",
        "\n",
        "        # Perform fitness calculation and update the population\n",
        "        ga_instance.population = ga_instance.cal_pop_fitness()\n",
        "        ga_instance.population = ga_instance.sort_population()\n",
        "\n",
        "        # Truncate the population to the original size\n",
        "        ga_instance.population = ga_instance.population[:population_size]\n",
        "\n",
        "        # Get the best solution from the GA\n",
        "        best_solution, best_solution_fitness, best_solution_idx = ga_instance.best_solution()\n",
        "\n",
        "        print(\"Generation:\", generation)\n",
        "        print(\"The current best chadscore for our youngsters is:\", best_solution_fitness)\n",
        "\n",
        "        # Save images every N generations (ex. 10)\n",
        "        if generation % 1 == 0:\n",
        "          # Convert the solution back to the original shape (1, 77, 768)\n",
        "          solution_reshaped = best_solution.reshape(1, 77, 768)\n",
        "          solution_reshaped = torch.tensor(solution_reshaped, dtype=torch.float32)\n",
        "\n",
        "          # Copy the tensor to CUDA device if 'device' is 'cuda'\n",
        "          if device == 'cuda':\n",
        "            solution_reshaped = solution_reshaped.to(device)\n",
        "\n",
        "          image = generate_images_from_embeddings(solution_reshaped, null_prompt)\n",
        "          pil_image = to_pil(image[0])\n",
        "          filename=f\"{IMAGES_DIR}/{generation}.png\"\n",
        "          pil_image.show()\n",
        "          pil_image.save(filename)\n",
        "\n",
        "    # Get the final best solution and images\n",
        "    best_solution, best_solution_fitness = ga_instance.best_solution()\n",
        "    best_solution = best_solution.reshape(1, 77, 768)  # Reshape the best solution to the correct shape\n",
        "    final_image = sd.generate_images_from_embeddings(embedded_prompt=best_solution, null_prompt=null_prompt)\n",
        "\n",
        "    # Save the final best solution images\n",
        "    final_filename=f\"{IMAGES_DIR}/final.png\"\n",
        "    pil_final = to_pil(final_image[0])\n",
        "    pil_final.show()\n",
        "    pil_final.save(final_filename)\n",
        "\n",
        "    return best_solution\n",
        "\n",
        "# Call the GA loop function with your initialized StableDiffusion model\n",
        "best_solution = genetic_algorithm_loop(sd, embedded_prompts, null_prompt)\n",
        "\n",
        "print(\"Best solution found!\")\n",
        "\n",
        "torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_final_solution.pt\"))\n",
        "print(\"Saving solution...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL-xzoZT8qvW"
      },
      "outputs": [],
      "source": [
        "# Clean unused loaded models\n",
        "del preprocess, image_features_clip_model, sd"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}