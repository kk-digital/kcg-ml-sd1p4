{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHdf0boc7Gch"
   },
   "source": [
    "## Generate 128, 64x64 random latent vectors - pass into auto encoder - time it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T14:25:57.985615Z",
     "iopub.status.busy": "2023-04-04T14:25:57.985199Z",
     "iopub.status.idle": "2023-04-04T14:25:59.679801Z",
     "shell.execute_reply": "2023-04-04T14:25:59.678458Z",
     "shell.execute_reply.started": "2023-04-04T14:25:57.985556Z"
    },
    "id": "vMgLxAbT7Gcy"
   },
   "outputs": [],
   "source": [
    "# Default env type is test\n",
    "# Change this when running on colab or kaggle\n",
    "ENV_TYPE = \"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if ENV_TYPE == \"TEST\":\n",
    "    model_path = \"/input/models\"\n",
    "    base_directory = \"../\"\n",
    "else:\n",
    "    # clone the repository\n",
    "    !git clone https://github.com/kk-digital/kcg-ml-sd1p4.git\n",
    "\n",
    "    # move to the repo\n",
    "    %cd kcg-ml-sd1p4/\n",
    "\n",
    "    model_path = \"./\"\n",
    "    # Get the current directory\n",
    "    base_directory = os.getcwd()\n",
    "    base_directory = os.path.join(base_directory, 'kcg-ml')\n",
    "    # download model weights\n",
    "    !wget https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\n",
    "\n",
    "# Insert the paths into sys.path\n",
    "sys.path.insert(0, base_directory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yyi5AledO7yd",
    "outputId": "18ed8506-932e-4ec4-b69c-d410c0debc65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Warning: GPU is not available, running on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-04T14:24:45.223671Z",
     "iopub.status.busy": "2023-04-04T14:24:45.222698Z",
     "iopub.status.idle": "2023-04-04T14:25:57.982513Z",
     "shell.execute_reply": "2023-04-04T14:25:57.981244Z",
     "shell.execute_reply.started": "2023-04-04T14:24:45.223629Z"
    },
    "id": "Z2GikaeW7Gcz"
   },
   "outputs": [],
   "source": [
    "#Install requirements \n",
    "!pip install diffusers==0.11.1\n",
    "!pip install transformers scipy ftfy accelerate\n",
    "!pip3 install labml\n",
    "!pip3 install labml-nn\n",
    "!pip3 install pytorch-lightning\n",
    "!pip install openai-clip\n",
    "!pip install Pillow==9.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T14:26:18.698951Z",
     "iopub.status.busy": "2023-04-04T14:26:18.698541Z",
     "iopub.status.idle": "2023-04-04T14:26:31.500876Z",
     "shell.execute_reply": "2023-04-04T14:26:31.499708Z",
     "shell.execute_reply.started": "2023-04-04T14:26:18.698904Z"
    },
    "id": "fWAN7oYQ7Gc0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Generate images using stable diffusion with a prompt\n",
    "summary: >\n",
    " Generate images using stable diffusion with a prompt\n",
    "---\n",
    "\n",
    "# Generate images using [stable diffusion](../index.html) with a prompt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from labml import lab, monit\n",
    "from stable_diffusion.latent_diffusion import LatentDiffusion\n",
    "from stable_diffusion.sampler.ddim import DDIMSampler\n",
    "from stable_diffusion.sampler.ddpm import DDPMSampler\n",
    "from stable_diffusion.utils.model import load_model, save_images, set_seed\n",
    "\n",
    "\n",
    "class Txt2Img:\n",
    "    \"\"\"\n",
    "    ### Text to image class\n",
    "    \"\"\"\n",
    "    model: LatentDiffusion\n",
    "\n",
    "    def __init__(self, *,\n",
    "                 checkpoint_path: Path,\n",
    "                 sampler_name: str,\n",
    "                 n_steps: int = 50,\n",
    "                 ddim_eta: float = 0.0,\n",
    "                 ):\n",
    "              self.load(checkpoint_path, sampler_name, n_steps, ddim_eta)\n",
    "\n",
    "    def load(self,\n",
    "             checkpoint_path: Path,\n",
    "             sampler_name: str,\n",
    "             n_steps: int = 50,\n",
    "             ddim_eta: float = 0.0,\n",
    "             ):\n",
    "        self.model = load_model(checkpoint_path)\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if sampler_name == 'ddim':\n",
    "            self.sampler = DDIMSampler(self.model,\n",
    "                                       n_steps=n_steps,\n",
    "                                       ddim_eta=ddim_eta)\n",
    "        elif sampler_name == 'ddpm':\n",
    "            self.sampler = DDPMSampler(self.model)\n",
    "\n",
    "    def unload(self):\n",
    "        self.model = None\n",
    "        self.sampler = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, *,\n",
    "                 dest_path: str,\n",
    "                 batch_size: int = 3,\n",
    "                 prompt: str,\n",
    "                 h: int = 512, w: int = 512,\n",
    "                 uncond_scale: float = 7.5,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param dest_path: is the path to store the generated images\n",
    "        :param batch_size: is the number of images to generate in a batch\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        :param h: is the height of the image\n",
    "        :param w: is the width of the image\n",
    "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
    "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "        \"\"\"\n",
    "        # Number of channels in the image\n",
    "        c = 4\n",
    "        # Image to latent space resolution reduction\n",
    "        f = 8\n",
    "\n",
    "        # Make a batch of prompts\n",
    "        prompts = batch_size * [prompt]\n",
    "\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n",
    "            if uncond_scale != 1.0:\n",
    "                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n",
    "            else:\n",
    "                un_cond = None\n",
    "            # Get the prompt embeddings\n",
    "            cond = self.model.get_text_conditioning(prompts)\n",
    "            # [Sample in the latent space](../sampler/index.html).\n",
    "            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n",
    "            x = self.sampler.sample(cond=cond,\n",
    "                                    shape=[batch_size, c, h // f, w // f],\n",
    "                                    uncond_scale=uncond_scale,\n",
    "                                    uncond_cond=un_cond)\n",
    "            # Decode the image from the [autoencoder](../model/autoencoder.html)\n",
    "            images = self.model.autoencoder_decode(x)\n",
    "\n",
    "        # Save images\n",
    "        save_images(images, dest_path, 'txt_')\n",
    "\n",
    "    # functions for pipeline\n",
    "    @torch.no_grad()\n",
    "    def generate_text_embeddings(self, prompt, batch_size=4, uncond_scale=7.5):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # Make a batch of prompts\n",
    "        prompts = batch_size * [prompt]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n",
    "            if uncond_scale != 1.0:\n",
    "                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n",
    "            else:\n",
    "                un_cond = None\n",
    "            # Get the prompt embeddings\n",
    "            cond = self.model.get_text_conditioning(prompts)\n",
    "\n",
    "        # return the embeddings\n",
    "        return cond, un_cond\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_latent_space(self, cond, un_cond, batch_size=4, uncond_scale=7.5, h=512, w=512):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # Number of channels in the image\n",
    "        c = 4\n",
    "        # Image to latent space resolution reduction\n",
    "        f = 8\n",
    "\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # [Sample in the latent space](../sampler/index.html).\n",
    "            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n",
    "            x = self.sampler.sample(cond=cond,\n",
    "                                    shape=[batch_size, c, h // f, w // f],\n",
    "                                    uncond_scale=uncond_scale,\n",
    "                                    uncond_cond=un_cond)\n",
    "        \n",
    "        # return the embeddings\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_image(self, x):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # Decode the image from the [autoencoder](../model/autoencoder.html)\n",
    "            image = self.model.autoencoder_decode(x)\n",
    "        \n",
    "        # return the embeddings\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVSZa_J9DOGL"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "class CLIPTextEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    ## CLIP Text Embedder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, version: str = \"openai/clip-vit-large-patch14\", max_length: int = 77):\n",
    "        \"\"\"\n",
    "        :param version: is the model version\n",
    "        :param max_length: is the max length of the tokenized prompt\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "        self.transformer = CLIPTextModel.from_pretrained(version).eval()\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "        # Move the transformer to the correct device\n",
    "        self.transformer = self.transformer.to(self.device)\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, prompts: List[str]):\n",
    "        \"\"\"\n",
    "        :param prompts: are the list of prompts to embed\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer.batch_encode_plus(\n",
    "            prompts, truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "        # Move input_ids to the correct device\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        return self.transformer(input_ids=input_ids).last_hidden_state\n",
    "\n",
    "\n",
    "# Example usage\n",
    "x = CLIPTextEmbedder()\n",
    "out = x.forward(prompts=[\"space marines\"])\n",
    "print(torch.Tensor.size(out))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOGFvNba7Gc2"
   },
   "source": [
    "## Return 128 image & Calculate time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T14:26:31.503115Z",
     "iopub.status.busy": "2023-04-04T14:26:31.502747Z",
     "iopub.status.idle": "2023-04-04T14:27:34.526457Z",
     "shell.execute_reply": "2023-04-04T14:27:34.525196Z",
     "shell.execute_reply.started": "2023-04-04T14:26:31.503063Z"
    },
    "id": "HhvzwX-J7Gc3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Initialize the Txt2Img class\n",
    "txt2img = Txt2Img(checkpoint_path=os.path.join(model_path, 'v1-5-pruned-emaonly.ckpt'),\n",
    "                  sampler_name='ddim',\n",
    "                  n_steps=50,\n",
    "                  ddim_eta=0.0)\n",
    "\n",
    "# Define parameters for generating images\n",
    "# N determines how many images will be generated in a single pass\n",
    "N = 128\n",
    "h = 64\n",
    "w = 64\n",
    "c = 4\n",
    "f = 8\n",
    "\n",
    "# Create random latent vectors\n",
    "latent_vectors = torch.randn(N, c, h // f, w // f).to(txt2img.device)\n",
    "\n",
    "# Time the decoding process\n",
    "start_time = time.time()\n",
    "\n",
    "# Decode images from latent vectors\n",
    "\n",
    "images = txt2img.generate_image(latent_vectors)\n",
    "\n",
    "# Calculate and print the time taken\n",
    "end_time = time.time()\n",
    "time_elapsed = end_time - start_time\n",
    "time_per_image = time_elapsed / N\n",
    "print(f\"number of images: {N}\")\n",
    "print(f\"Time taken to generate images: {time_elapsed:.4f} seconds\")\n",
    "print(f\"Time taken per image: {time_per_image:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU47NlD58tUv"
   },
   "source": [
    "# Return 128 random prompt & Calculate Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpFmlBpL_GOi"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "phrase_list = ['2d', 'pixel art', 'cave', 'scifi', 'side scrolling', 'chibi', 'waifu', 'space ship', 'desert', 'city', 'wasteland', 'mega structure', 'steal', 'stone', 'rock']\n",
    "\n",
    "def prompt_generator(phrase_list, prompt_word_length=32, prompt_number=1):\n",
    "    prompts = []\n",
    "    for i in range(prompt_number):\n",
    "        prompt = ''\n",
    "        while len(prompt) < prompt_word_length:\n",
    "            phrase = random.choice(phrase_list)\n",
    "            if len(prompt) + len(phrase) + 1 <= prompt_word_length:\n",
    "                prompt += phrase + ' '\n",
    "            else:\n",
    "                break\n",
    "        prompts.append(prompt.strip())\n",
    "    return prompts\n",
    "\n",
    "start_time = time.time()\n",
    "prompts = prompt_generator(phrase_list, prompt_word_length=32, prompt_number=128)\n",
    "end_time = time.time()\n",
    "\n",
    "print('Generated prompts:')\n",
    "for prompt in prompts:\n",
    "    print(prompt)\n",
    "\n",
    "print('Execution time:', end_time - start_time, 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeosMqOnRbRP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
