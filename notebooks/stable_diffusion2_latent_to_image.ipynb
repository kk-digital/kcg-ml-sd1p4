{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DyFpBDlbd7C"
      },
      "source": [
        "# Converting a latent space vector to an image quickly\n",
        "## [Converting-latents-to-rgb-without-encoding](https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204)\n",
        "\n",
        "This notebooks demonstrated a script in ./ga/latent_to_image.py, which as it name implies, takes a latent space vector (gathered from denoising an embedding using stable diffusion), and converts it to a usable PIL image.\n",
        "\n",
        "**The script also provides a method to decode the latent by loading and using Stable Diffusion's autoencoder, though it is not demonstrated on this notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1iumXeh69Pa"
      },
      "outputs": [],
      "source": [
        "ENV_TYPE = \"TES1T\"\n",
        "\n",
        "if(ENV_TYPE != \"TEST\"):\n",
        "  !git clone \"https://github.com/kk-digital/kcg-ml-sd1p4.git\"\n",
        "  %cd kcg-ml-sd1p4\n",
        "  !pip3 install -r requirements.txt\n",
        "  exit()\n",
        "  base_directory = \"./\"\n",
        "else:\n",
        "  base_directory = \"../\"\n",
        "\n",
        "# Magical check for fixing all of our directory issues\n",
        "import subprocess\n",
        "output = subprocess.check_output([\"pwd\"], universal_newlines=True)\n",
        "if \"notebooks\" in output:\n",
        "    %cd ..\n",
        "del output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UMgSD7xTbd7F"
      },
      "outputs": [],
      "source": [
        "# Check for dependency needed for using OpenCV\n",
        "import subprocess\n",
        "\n",
        "result = subprocess.run(['dpkg', '-s', 'libgl1-mesa-glx'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "# If the package is not installed, install it\n",
        "if 'is not installed and no information is available' in result.stderr:\n",
        "    print(\"Installing libgl, which is needed to run the GA script.\")\n",
        "    subprocess.run([\"apt\", \"update\"])\n",
        "    subprocess.run([\"apt\", \"install\", \"libgl1-mesa-glx\"])\n",
        "else:\n",
        "    print(\"Package 'libgl1-mesa-glx' is already installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbV9eAw169Pc"
      },
      "outputs": [],
      "source": [
        "!python3 ./download_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVGtkQBh69Pc"
      },
      "outputs": [],
      "source": [
        "!python3 ./process_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6917Ijqv69Pd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import time\n",
        "import shutil\n",
        "from torchvision.transforms import ToPILImage\n",
        "from os.path import join\n",
        "\n",
        "base_directory = \"../\"\n",
        "sys.path.insert(0, base_directory)\n",
        "\n",
        "from stable_diffusion.model_paths import *\n",
        "from configs.model_config import ModelPathConfig\n",
        "from stable_diffusion.utils_backend import *\n",
        "from stable_diffusion.utils_image import *\n",
        "from stable_diffusion.utils_model import *\n",
        "from stable_diffusion.stable_diffusion import StableDiffusion\n",
        "from utility.labml import monit\n",
        "\n",
        "\n",
        "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
        "output_directory = join(output_base_dir, \"latent_to_image/\")\n",
        "\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(output_directory)\n",
        "except Exception as e:\n",
        "    print(e, \"\\n\", \"Creating the path...\")\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "else:\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "\n",
        "def to_pil(image):\n",
        "    return ToPILImage()(torch.clamp((image + 1.0) / 2.0, min=0.0, max=1.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k44DDtTq69Pe"
      },
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "base_dir = os.getcwd()\n",
        "sys.path.insert(0, base_dir)\n",
        "\n",
        "batch_size = 1\n",
        "model_config = ModelPathConfig()\n",
        "pt = IODirectoryTree(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvURFyxE69Pe"
      },
      "outputs": [],
      "source": [
        "# initialize an empty stable diffusion class\n",
        "stable_diffusion = StableDiffusion(device=device)\n",
        "get_memory_status(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tgS-j5U69Pf"
      },
      "outputs": [],
      "source": [
        "# initialize an empty latent diffusion model; it returns self.model\n",
        "# then load the clip text embedder from the path `pt.embedder_path` with .load_clip_embedder()\n",
        "# it returns the clip embedder, so you can chain a .load_submodels() to load the text embedder submodels\n",
        "\n",
        "stable_diffusion.quick_initialize().load_clip_embedder().load_submodels()\n",
        "get_memory_status(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJLRPgfZ69Pg"
      },
      "outputs": [],
      "source": [
        "stable_diffusion.model.clip_embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xKekGteH69Ph"
      },
      "outputs": [],
      "source": [
        "# get the embedding for a prompt\n",
        "prompt_embedding = stable_diffusion.model.clip_embedder(\n",
        "    [\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\"]\n",
        ")\n",
        "null_prompt = stable_diffusion.model.clip_embedder([\"\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IolCk_X269Pi"
      },
      "outputs": [],
      "source": [
        "get_memory_status(device)\n",
        "prompt_embedding.shape, null_prompt.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua1Lzdu769Pi"
      },
      "outputs": [],
      "source": [
        "# we don't need the embedder anymore, so we can unload it\n",
        "stable_diffusion.model.unload_clip_embedder()\n",
        "get_memory_status(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l7Y_OYAx69Pj"
      },
      "outputs": [],
      "source": [
        "# let's save the prompt embedding\n",
        "torch.save(prompt_embedding, join(output_directory, \"prompt_embedding.pt\"))\n",
        "torch.save(null_prompt, join(output_directory, \"null_prompt.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BO2iG1z169Ps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "stable_diffusion.model.load_unet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-kx-5wT69Pt"
      },
      "outputs": [],
      "source": [
        "latent_space_vector = stable_diffusion.generate_images_latent_from_embeddings(\n",
        "    embedded_prompt=prompt_embedding, null_prompt=null_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksyv55ZV69Pt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from ga.latent_to_image import latent_to_pil_image\n",
        "\n",
        "# Example usage of the function with the 'images' variable\n",
        "pil_image = latent_to_pil_image(latent_space_vector, size=512)\n",
        "pil_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ga.latent_to_image import generate_image_from_latent\n",
        "\n",
        "final = generate_image_from_latent(stable_diffusion, latent_space_vector)\n",
        "final"
      ],
      "metadata": {
        "id": "5XkTcvdThUoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}