{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical Interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArNSnrXw459R",
    "outputId": "c7fb24fa-9493-4bf6-9c00-d96e3bdc1a9f",
    "ExecuteTime": {
     "start_time": "2023-06-19T17:22:10.336208Z",
     "end_time": "2023-06-19T17:22:20.030983Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install diffusers==0.11.1\n",
    "!pip install transformers scipy ftfy accelerate\n",
    "!pip3 install labml\n",
    "!pip3 install labml-nn\n",
    "!pip3 install pytorch-lightning\n",
    "!pip install Pillow==9.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Default env type is test\n",
    "# Change this when running on colab or kaggle\n",
    "ENV_TYPE = \"TEST\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print ('[WARNING] CUDA/GPU is not available! Compute-intensive scripts on this notebook will be run on CPU.')\n",
    "    device =  \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:22:20.035326Z",
     "end_time": "2023-06-19T17:22:20.038754Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pytest\n",
    "if ENV_TYPE == \"TEST\":\n",
    "    model_path = \"/input/models\"\n",
    "    base_directory = \"../\"\n",
    "else:\n",
    "    # clone the repository\n",
    "    !git clone https://github.com/kk-digital/kcg-ml-sd1p4.git\n",
    "\n",
    "    # move to the repo\n",
    "    %cd kcg-ml-sd1p4/\n",
    "\n",
    "    model_path = \"./\"\n",
    "    # Get the current directory\n",
    "    base_directory = os.getcwd()\n",
    "    base_directory = os.path.join(base_directory, 'kcg-ml')\n",
    "    # download model weights\n",
    "    !wget https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\n",
    "\n",
    "# Insert the paths into sys.path\n",
    "sys.path.insert(0, base_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-19T17:22:20.043280Z",
     "end_time": "2023-06-19T17:22:20.046644Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:46:41.566401Z",
     "iopub.status.busy": "2023-04-05T11:46:41.565969Z",
     "iopub.status.idle": "2023-04-05T11:46:41.586869Z",
     "shell.execute_reply": "2023-04-05T11:46:41.584269Z",
     "shell.execute_reply.started": "2023-04-05T11:46:41.566360Z"
    },
    "id": "CVUMEfU4XhE2",
    "ExecuteTime": {
     "start_time": "2023-06-19T17:22:20.061837Z",
     "end_time": "2023-06-19T17:22:20.067822Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Generate images using stable diffusion with a prompt\n",
    "summary: >\n",
    " Generate images using stable diffusion with a prompt\n",
    "---\n",
    "\n",
    "# Generate images using [stable diffusion](../index.html) with a prompt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from labml import lab, monit\n",
    "from stable_diffusion.latent_diffusion import LatentDiffusion\n",
    "from stable_diffusion.sampler.ddim import DDIMSampler\n",
    "from stable_diffusion.sampler.ddpm import DDPMSampler\n",
    "from stable_diffusion.utils.model import load_model, save_images, save_image\n",
    "\n",
    "\n",
    "class Txt2Img:\n",
    "    \"\"\"\n",
    "    ### Text to image class\n",
    "    \"\"\"\n",
    "    model: LatentDiffusion\n",
    "\n",
    "    def __init__(self, *,\n",
    "                 checkpoint_path: Path,\n",
    "                 sampler_name: str,\n",
    "                 n_steps: int = 50,\n",
    "                 ddim_eta: float = 0.0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param checkpoint_path: is the path of the checkpoint\n",
    "        :param sampler_name: is the name of the [sampler](../sampler/index.html)\n",
    "        :param n_steps: is the number of sampling steps\n",
    "        :param ddim_eta: is the [DDIM sampling](../sampler/ddim.html) $\\eta$ constant\n",
    "        \"\"\"\n",
    "        # Load [latent diffusion model](../latent_diffusion.html)\n",
    "        self.model = load_model(checkpoint_path)\n",
    "        # Get device\n",
    "        self.device = torch.device(device)\n",
    "        # Move the model to device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Initialize [sampler](../sampler/index.html)\n",
    "        if sampler_name == 'ddim':\n",
    "            self.sampler = DDIMSampler(self.model,\n",
    "                                       n_steps=n_steps,\n",
    "                                       ddim_eta=ddim_eta)\n",
    "        elif sampler_name == 'ddpm':\n",
    "            self.sampler = DDPMSampler(self.model)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, *,\n",
    "                 dest_path: str,\n",
    "                 batch_size: int = 3,\n",
    "                 prompt: str,\n",
    "                 h: int = 512, w: int = 512,\n",
    "                 uncond_scale: float = 7.5,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param dest_path: is the path to store the generated images\n",
    "        :param batch_size: is the number of images to generate in a batch\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        :param h: is the height of the image\n",
    "        :param w: is the width of the image\n",
    "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
    "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "        \"\"\"\n",
    "        # Number of channels in the image\n",
    "        c = 4\n",
    "        # Image to latent space resolution reduction\n",
    "        f = 8\n",
    "\n",
    "        # Make a batch of prompts\n",
    "        prompts = batch_size * [prompt]\n",
    "\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n",
    "            if uncond_scale != 1.0:\n",
    "                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n",
    "            else:\n",
    "                un_cond = None\n",
    "            # Get the prompt embeddings\n",
    "            cond = self.model.get_text_conditioning(prompts)\n",
    "            # [Sample in the latent space](../sampler/index.html).\n",
    "            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n",
    "            x = self.sampler.sample(cond=cond,\n",
    "                                    shape=[batch_size, c, h // f, w // f],\n",
    "                                    uncond_scale=uncond_scale,\n",
    "                                    uncond_cond=un_cond)\n",
    "            # Decode the image from the [autoencoder](../model/autoencoder.html)\n",
    "            images = self.model.autoencoder_decode(x)\n",
    "\n",
    "        # Save images\n",
    "        save_images(images, dest_path, 'jpeg')\n",
    "\n",
    "    # functions for pipeline\n",
    "    @torch.no_grad()\n",
    "    def generate_text_embeddings(self, prompt, batch_size=4, uncond_scale=7.5):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # Make a batch of prompts\n",
    "        prompts = batch_size * [prompt]\n",
    "\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n",
    "            if uncond_scale != 1.0:\n",
    "                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n",
    "            else:\n",
    "                un_cond = None\n",
    "            # Get the prompt embeddings\n",
    "            cond = self.model.get_text_conditioning(prompts)\n",
    "        \n",
    "        # return the embeddings\n",
    "        return cond, un_cond\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_latent_space(self, cond, un_cond, batch_size=4, uncond_scale=7.5, h=512, w=512):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # Number of channels in the image\n",
    "        c = 4\n",
    "        # Image to latent space resolution reduction\n",
    "        f = 8\n",
    "\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # [Sample in the latent space](../sampler/index.html).\n",
    "            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n",
    "            x = self.sampler.sample(cond=cond,\n",
    "                                    shape=[batch_size, c, h // f, w // f],\n",
    "                                    uncond_scale=uncond_scale,\n",
    "                                    uncond_cond=un_cond)\n",
    "        \n",
    "        # return the embeddings\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_image(self, x):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # Decode the image from the [autoencoder](../model/autoencoder.html)\n",
    "            image = self.model.autoencoder_decode(x)\n",
    "        \n",
    "        # return the embeddings\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5TZk3FGsvAf",
    "outputId": "fea68e95-987a-4595-ba67-e551d4d90c45",
    "ExecuteTime": {
     "start_time": "2023-06-19T17:22:20.070532Z",
     "end_time": "2023-06-19T17:22:36.655588Z"
    }
   },
   "outputs": [],
   "source": [
    "# create an instance of the class\n",
    "sampler_name = 'ddim'\n",
    "steps = 50\n",
    "batch_size = 4\n",
    "scale = 7.5\n",
    "uncond_scale = 7.5\n",
    "txt2img = Txt2Img(checkpoint_path=os.path.join(model_path, 'sd-v1-4.ckpt'),\n",
    "                      sampler_name=sampler_name,\n",
    "                      n_steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:45:41.184626Z",
     "iopub.status.busy": "2023-04-05T11:45:41.183995Z",
     "iopub.status.idle": "2023-04-05T11:45:43.161687Z",
     "shell.execute_reply": "2023-04-05T11:45:43.160653Z",
     "shell.execute_reply.started": "2023-04-05T11:45:41.184586Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt1 = \"a photograph of an astronaut riding harley davidson\"\n",
    "prompt2 = \"a photograph of an astronaut surfing\"\n",
    "\n",
    "\n",
    "# Get the embeddings for both prompts\n",
    "embeddings1 = txt2img.generate_text_embeddings(prompt=prompt1, batch_size=batch_size, uncond_scale=uncond_scale)\n",
    "embeddings2 = txt2img.generate_text_embeddings(prompt=prompt2, batch_size=batch_size, uncond_scale=uncond_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:50:20.167815Z",
     "iopub.status.busy": "2023-04-05T11:50:20.167080Z",
     "iopub.status.idle": "2023-04-05T11:52:13.844940Z",
     "shell.execute_reply": "2023-04-05T11:52:13.842956Z",
     "shell.execute_reply.started": "2023-04-05T11:50:20.167774Z"
    },
    "id": "8tyZtbFKuAGu"
   },
   "outputs": [],
   "source": [
    "#Get the latents fro the both prompts\n",
    "\n",
    "latent_space1 = txt2img.generate_latent_space(cond=embeddings1[0], un_cond=embeddings1[1], batch_size=batch_size, uncond_scale=uncond_scale, h=512, w=512)\n",
    "latent_space2 = txt2img.generate_latent_space(cond=embeddings2[0], un_cond=embeddings2[1], batch_size=batch_size, uncond_scale=uncond_scale, h=512, w=512)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  InterpolateLatentSpherical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:53:18.248687Z",
     "iopub.status.busy": "2023-04-05T11:53:18.247954Z",
     "iopub.status.idle": "2023-04-05T11:53:18.256017Z",
     "shell.execute_reply": "2023-04-05T11:53:18.254879Z",
     "shell.execute_reply.started": "2023-04-05T11:53:18.248646Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate_latent_spherical(latent1, latent2, alpha):\n",
    "    latent1_flat, latent2_flat = latent1.view(latent1.shape[0], -1), latent2.view(latent2.shape[0], -1)\n",
    "    latent1_norm, latent2_norm = torch.norm(latent1_flat, dim=-1), torch.norm(latent2_flat, dim=-1)\n",
    "    dot_product = torch.sum(latent1_flat * latent2_flat, dim=-1) / (latent1_norm * latent2_norm)\n",
    "    \n",
    "    omega, sin_omega = torch.acos(dot_product), torch.sin(torch.acos(dot_product))\n",
    "    alpha_weights, one_minus_alpha_weights = torch.sin(alpha * omega) / sin_omega, torch.sin((1 - alpha) * omega) / sin_omega\n",
    "    \n",
    "    return latent1 * one_minus_alpha_weights[:, None, None, None] + latent2 * alpha_weights[:, None, None, None]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:53:20.210388Z",
     "iopub.status.busy": "2023-04-05T11:53:20.209638Z",
     "iopub.status.idle": "2023-04-05T11:53:20.215391Z",
     "shell.execute_reply": "2023-04-05T11:53:20.214245Z",
     "shell.execute_reply.started": "2023-04-05T11:53:20.210349Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_image(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:53:23.169491Z",
     "iopub.status.busy": "2023-04-05T11:53:23.169112Z",
     "iopub.status.idle": "2023-04-05T11:53:49.473772Z",
     "shell.execute_reply": "2023-04-05T11:53:49.472694Z",
     "shell.execute_reply.started": "2023-04-05T11:53:23.169455Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = './output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "num_interpolations = 2\n",
    "alphas = torch.tensor([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]).to(txt2img.device)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    interpolated_latent_spherical = interpolate_latent_spherical(latent_space1, latent_space2, alpha)\n",
    "        \n",
    "    # Generate images from the interpolated latent\n",
    "    with torch.no_grad():\n",
    "        images = txt2img.model.autoencoder_decode(interpolated_latent_spherical)\n",
    "        file_name = f\"interpolated_00000{i}.jpeg\"\n",
    "        img_path = os.path.join(output_dir, file_name)\n",
    "        save_image(images, img_path)\n",
    "\n",
    "    # Display the first and last images\n",
    "    display_image(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deAXKGQP3_ix"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
