{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kk-digital/kcg-ml-sd1p4\n",
    "%cd kcg-ml-sd1p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "from torchvision.transforms import ToPILImage\n",
    "from os.path import join\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "from stable_diffusion.constants import IODirectoryTree\n",
    "from stable_diffusion.utils_backend import *\n",
    "from stable_diffusion.utils_image import *\n",
    "from stable_diffusion.utils_model import *\n",
    "from stable_diffusion.utils_logger import *\n",
    "from stable_diffusion.stable_diffusion import StableDiffusion\n",
    "from utility.labml import monit\n",
    "\n",
    "\n",
    "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
    "output_directory = join(output_base_dir, \"denoising/\")\n",
    "\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_directory)\n",
    "except Exception as e:\n",
    "    print(e, \"\\n\", \"Creating the path...\")\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def to_pil(image):\n",
    "    return ToPILImage()(torch.clamp((image + 1.0) / 2.0, min=0.0, max=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "pt = IODirectoryTree(base_directory=base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty stable diffusion class\n",
    "stable_diffusion = StableDiffusion(device=device)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty latent diffusion model; it returns self.model\n",
    "# then load the clip text embedder from the path `pt.embedder_path` with .load_clip_embedder()\n",
    "# it returns the clip embedder, so you can chain a .load_submodels() to load the text embedder submodels\n",
    "\n",
    "stable_diffusion.quick_initialize().load_clip_embedder(**pt.embedder).load_submodels(\n",
    "    **pt.embedder_submodels\n",
    ")\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion.model.clip_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedding for a prompt\n",
    "prompt_embedding = stable_diffusion.model.clip_embedder(\n",
    "    [\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\"]\n",
    ")\n",
    "null_prompt = stable_diffusion.model.clip_embedder([\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_status()\n",
    "prompt_embedding.shape, null_prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need the embedder anymore, so we can unload it\n",
    "stable_diffusion.model.unload_clip_embedder()\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the prompt embedding\n",
    "torch.save(prompt_embedding, join(output_directory, \"prompt_embedding.pt\"))\n",
    "torch.save(null_prompt, join(output_directory, \"null_prompt.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the latent diffusion class has a method to load the unet, since it is a submodel of it. it returns the unet model, wrapped in a DiffusionWrapper class.\n",
    "# it is accessible as self.model.model or through the alias self.model.unet\n",
    "stable_diffusion.model.load_unet(**pt.unet)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a latent representation, we know beforehand that the latent space is shaped as (1, 4, 64, 64)\n",
    "initial_latent = torch.randn(1, 4, 64, 64, device=device)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a timestep for this sample\n",
    "time_step = torch.tensor([15.0]).to(device)\n",
    "time_step.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(\n",
    "    stable_diffusion: StableDiffusion,\n",
    "    shape: List[int],\n",
    "    cond: torch.Tensor,\n",
    "    repeat_noise: bool = False,\n",
    "    temperature: float = 1.0,\n",
    "    x_last: Optional[torch.Tensor] = None,\n",
    "    uncond_scale: float = 7.5,\n",
    "    uncond_cond: Optional[torch.Tensor] = None,\n",
    "    skip_steps: int = 0,\n",
    "    noise_fn=torch.randn,\n",
    "):\n",
    "    \"\"\"\n",
    "    ### Sampling Loop\n",
    "    :param shape: is the shape of the generated images in the\n",
    "        form `[batch_size, channels, height, width]`\n",
    "    :param cond: is the conditional embeddings $c$\n",
    "    :param temperature: is the noise temperature (random noise gets multiplied by this)\n",
    "    :param x_last: is $x_{\\tau_S}$. If not provided random noise will be used.\n",
    "    :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
    "        $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "    :param uncond_cond: is the conditional embedding for empty prompt $c_u$\n",
    "    :param skip_steps: is the number of time steps to skip $i'$. We start sampling from $S - i'$.\n",
    "        And `x_last` is then $x_{\\tau_{S - i'}}$.\n",
    "    \"\"\"\n",
    "    # Get device and batch size\n",
    "    set_seed(0)\n",
    "    latents = []\n",
    "    device = stable_diffusion.sampler.model.device\n",
    "    bs = shape[0]\n",
    "    # Get $x_{\\tau_S}$\n",
    "    x = x_last if x_last is not None else noise_fn(shape, device=device)\n",
    "    latents.append(x)\n",
    "    # Time steps to sample at $\\tau_{S - i'}, \\tau_{S - i' - 1}, \\dots, \\tau_1$\n",
    "    time_steps = np.flip(stable_diffusion.sampler.time_steps)[skip_steps:]\n",
    "    for i, step in monit.enum(\"Sample\", time_steps):\n",
    "        # Index $i$ in the list $[\\tau_1, \\tau_2, \\dots, \\tau_S]$\n",
    "        index = len(time_steps) - i - 1\n",
    "        # Time step $\\tau_i$\n",
    "        ts = x.new_full((bs,), step, dtype=torch.long)\n",
    "        # Sample $x_{\\tau_{i-1}}$\n",
    "        x, pred_x0, e_t = stable_diffusion.sampler.p_sample(\n",
    "            x,\n",
    "            cond,\n",
    "            ts,\n",
    "            step,\n",
    "            index=index,\n",
    "            repeat_noise=repeat_noise,\n",
    "            temperature=temperature,\n",
    "            uncond_scale=uncond_scale,\n",
    "            uncond_cond=uncond_cond,\n",
    "            noise_fn=noise_fn,\n",
    "        )\n",
    "        latents.append(x)\n",
    "    # Return $x_0$\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast = get_autocast()\n",
    "with autocast:\n",
    "    with torch.no_grad():\n",
    "        latents = sample(\n",
    "            stable_diffusion=stable_diffusion,\n",
    "            shape=[1, 4, 64, 64],\n",
    "            cond=prompt_embedding,\n",
    "            uncond_cond=null_prompt,\n",
    "            x_last=initial_latent,\n",
    "            noise_fn=torch.randn,\n",
    "        )\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.cat(latents[1:], dim=0)\n",
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torchvision.utils.make_grid(\n",
    "    grid,\n",
    "    nrow=10,\n",
    "    normalize=False,\n",
    "    range=(-1, 1),\n",
    "    scale_each=True,\n",
    "    pad_value=0,\n",
    ")\n",
    "dim_grid_image = to_pil(grid)\n",
    "dim_grid_image.save(join(output_directory, f\"denoising_process.png\"))\n",
    "dim_grid_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion.model.unload_submodels()\n",
    "torch.cuda.empty_cache()\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_decoded = latents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion.model.load_autoencoder(**pt.autoencoder).load_decoder(**pt.decoder)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_image = stable_diffusion.decode(to_be_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil(decoded_image.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "stable_diffusion.model.load_unet(**pt.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = stable_diffusion.generate_images_from_embeddings(\n",
    "    embedded_prompt=prompt_embedding, null_prompt=null_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "to_pil(images.squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
