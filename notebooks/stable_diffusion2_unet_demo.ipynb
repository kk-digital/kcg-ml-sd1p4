{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPeLWWZ3Bl6S"
   },
   "outputs": [],
   "source": [
    "ENV_TYPE = \"TEST\"\n",
    "\n",
    "if(ENV_TYPE != \"TEST\"):\n",
    "  !git clone \"https://github.com/kk-digital/kcg-ml-sd1p4.git\"\n",
    "  %cd kcg-ml-sd1p4\n",
    "  !pip3 install -r requirements.txt\n",
    "  exit()\n",
    "  base_directory = \"./\"\n",
    "else:\n",
    "  base_directory = \"../\"\n",
    "\n",
    "# Magical check for fixing all of our directory issues\n",
    "import subprocess\n",
    "output = subprocess.check_output([\"pwd\"], universal_newlines=True)\n",
    "if \"notebooks\" in output:\n",
    "    %cd ..\n",
    "del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJZg4VBIBl6T"
   },
   "outputs": [],
   "source": [
    "!python3 ./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UffPgdSXBl6U"
   },
   "outputs": [],
   "source": [
    "!python3 ./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mNA16UmBl6V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "from torchvision.transforms import ToPILImage\n",
    "from os.path import join\n",
    "\n",
    "base_directory = \"./\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
    "output_directory = join(output_base_dir, \"unet/\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_directory)\n",
    "except Exception as e:\n",
    "    print(e, \"\\n\", \"Creating the path...\")\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "from stable_diffusion.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.utils_backend import *\n",
    "from stable_diffusion.utils_image import *\n",
    "from stable_diffusion.utils_model import *\n",
    "from stable_diffusion.model_paths import IODirectoryTree\n",
    "from configs.model_config import ModelPathConfig\n",
    "\n",
    "\n",
    "to_pil = lambda image: ToPILImage()(torch.clamp((image + 1.0) / 2.0, min=0.0, max=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHAxNInpBl6V"
   },
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eg-qokHHBl6W"
   },
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "sys.path.insert(0, base_dir)\n",
    "\n",
    "batch_size = 1\n",
    "model_config = ModelPathConfig()\n",
    "pt = IODirectoryTree(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJvFH4UeBl6W"
   },
   "outputs": [],
   "source": [
    "# initialize an empty stable diffusion class\n",
    "stable_diffusion = StableDiffusion(device=device)\n",
    "get_memory_status(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3ssBW6mBl6X"
   },
   "outputs": [],
   "source": [
    "# initialize an empty latent diffusion model; it returns self.model\n",
    "stable_diffusion.quick_initialize()\n",
    "get_memory_status(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76BxrUt6Bl6X"
   },
   "outputs": [],
   "source": [
    "# to use the unet we first need conditioning context. we can use the clip embedder to get it.\n",
    "stable_diffusion.model.load_clip_embedder()\n",
    "get_memory_status(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k82tki8zBl6Y"
   },
   "outputs": [],
   "source": [
    "# load the embedder submodels, tokenizer and transformer\n",
    "stable_diffusion.model.clip_embedder.load_submodels()\n",
    "get_memory_status(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbq-f2zvBl6Z"
   },
   "outputs": [],
   "source": [
    "stable_diffusion.model.clip_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DwHY-ADBl6Z"
   },
   "outputs": [],
   "source": [
    "# get the embedding for a prompt\n",
    "prompt_embedding = stable_diffusion.model.clip_embedder(\n",
    "    [\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WO2vjbTuBl6a"
   },
   "outputs": [],
   "source": [
    "get_memory_status(device)\n",
    "prompt_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGjwDHggBl6a"
   },
   "outputs": [],
   "source": [
    "# we don't need the embedder anymore, so we can unload it\n",
    "stable_diffusion.model.unload_clip_embedder()\n",
    "get_memory_status(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKfaGIdVBl6a"
   },
   "outputs": [],
   "source": [
    "# let's save the prompt embedding\n",
    "torch.save(prompt_embedding, join(output_base_dir, \"prompt_embedding.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA_rZPW_Bl6a"
   },
   "outputs": [],
   "source": [
    "# the latent diffusion class has a method to load the unet, since it is a submodel of it. it returns the unet model, wrapped in a DiffusionWrapper class.\n",
    "# it is accessible as self.model.model or through the alias self.model.unet\n",
    "stable_diffusion.model.load_unet()\n",
    "get_memory_status(device)\n",
    "stable_diffusion.model.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNVPItnkBl6a"
   },
   "outputs": [],
   "source": [
    "# load an encoded image and get its shape\n",
    "encoded_img = torch.load(join(output_base_dir, \"encoded_img_tensor.pt\")).to(device)\n",
    "encoded_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_q-uoBmoBl6a"
   },
   "outputs": [],
   "source": [
    "# sample a latent representation of same shape as the encoded image\n",
    "sample = torch.randn_like(encoded_img)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o6j9yAABl6a"
   },
   "outputs": [],
   "source": [
    "# define a timestep for this sample\n",
    "time_step = torch.tensor([0.0]).to(device)\n",
    "time_step.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GllijQwYBl6b"
   },
   "outputs": [],
   "source": [
    "# predict noise with the unet\n",
    "unet_output = stable_diffusion.model.unet(sample, time_step, prompt_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSZfV8wbBl6b"
   },
   "outputs": [],
   "source": [
    "get_memory_status(device)\n",
    "unet_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtAflv2wBl6b"
   },
   "outputs": [],
   "source": [
    "grid = torchvision.utils.make_grid(\n",
    "    unet_output.permute(1, 0, 2, 3),\n",
    "    nrow=2,\n",
    "    normalize=False,\n",
    "    range=(-1, 1),\n",
    "    scale_each=True,\n",
    "    pad_value=0,\n",
    ")\n",
    "dim_grid_image = to_pil(grid)\n",
    "dim_grid_image.save(join(output_directory, f\"unet_output.png\"))\n",
    "dim_grid_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1S29ITkBl6b"
   },
   "outputs": [],
   "source": [
    "stable_diffusion.model.unload_submodels()\n",
    "torch.cuda.empty_cache()\n",
    "get_memory_status(device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
