{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRE0dVo7hz2N"
   },
   "outputs": [],
   "source": [
    "# This notebooks 'tests' the current codebase to see if it can grab a series of prompts, convert them to embeddings, save them, generate images from them, and run clip to get back the same prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0i1Y2FMdQIt"
   },
   "outputs": [],
   "source": [
    "ENV_TYPE = \"TEST\"\n",
    "\n",
    "if(ENV_TYPE != \"TEST\"):\n",
    "  !git clone \"https://github.com/kk-digital/kcg-ml-sd1p4.git\"\n",
    "  %cd kcg-ml-sd1p4\n",
    "  !pip3 install -r requirements.txt\n",
    "  exit()\n",
    "else:\n",
    "  %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ97cuJidQIv"
   },
   "outputs": [],
   "source": [
    "!python3 ./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "on99PlsbdQIv"
   },
   "outputs": [],
   "source": [
    "!python3 ./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6b7nYMcdQIw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import configparser\n",
    "from typing import List\n",
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "base_dir = \"./\"\n",
    "sys.path.insert(0, base_dir)\n",
    "\n",
    "\n",
    "from stable_diffusion.model.clip_text_embedder import CLIPTextEmbedder\n",
    "from stable_diffusion.model.clip_image_encoder import CLIPImageEncoder\n",
    "from stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.constants import IODirectoryTree\n",
    "from stable_diffusion.utils_backend import (\n",
    "    get_device,\n",
    "    get_memory_status,\n",
    ")\n",
    "from stable_diffusion.utils_image import (\n",
    "    to_pil,\n",
    "    save_image_grid,\n",
    "    show_image_grid,\n",
    ")\n",
    "\n",
    "# EMBEDDED_PROMPTS_DIR = os.path.abspath(join(base_dir, \"/input/embedded_prompts/\"))\n",
    "EMBEDDED_PROMPTS_DIR = os.path.abspath(join(base_dir, \"./input/embedded_prompts/\"))\n",
    "print(EMBEDDED_PROMPTS_DIR)\n",
    "OUTPUT_DIR = os.path.abspath(\n",
    "    join(base_dir, \"./output/sd2-notebook/disturbed_embeddings/\")\n",
    ")\n",
    "IMAGES_DIR = os.path.abspath(join(OUTPUT_DIR, \"images/\"))\n",
    "FEATURES_DIR = os.path.abspath(join(OUTPUT_DIR, \"features/\"))\n",
    "print(OUTPUT_DIR)\n",
    "print(IMAGES_DIR)\n",
    "print(FEATURES_DIR)\n",
    "NULL_PROMPT = \"\"\n",
    "PROMPT = (\n",
    "    \"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\"\n",
    ")\n",
    "NUM_ITERATIONS = 16\n",
    "SEED = 2982\n",
    "NOISE_MULTIPLIER = 0.01\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# DEVICE = input(\"Set device: 'cuda:i' or 'cpu'\")\n",
    "DEVICE = None\n",
    "DEVICE = get_device()\n",
    "\n",
    "config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation())\n",
    "config.read(os.path.join(base_dir, \"config.ini\"))\n",
    "config['BASE']['BASE_DIRECTORY'] = base_dir\n",
    "config[\"BASE\"].get('base_io_directory')\n",
    "\n",
    "pt = IODirectoryTree(base_io_directory_prefix = config[\"BASE\"].get('base_io_directory_prefix'), base_directory=base_dir)\n",
    "pt.create_directory_tree_folders()\n",
    "\n",
    "\n",
    "os.makedirs(EMBEDDED_PROMPTS_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8NMl8edfdQIx"
   },
   "outputs": [],
   "source": [
    "def calculate_sha256(tensor):\n",
    "    if tensor.device == \"cpu\":\n",
    "        tensor_bytes = tensor.numpy().tobytes()  # Convert tensor to a byte array\n",
    "    else:\n",
    "        tensor_bytes = tensor.cpu().numpy().tobytes()  # Convert tensor to a byte array\n",
    "    sha256_hash = hashlib.sha256(tensor_bytes)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "\n",
    "def embed_and_save_prompts(prompts: list, null_prompt=NULL_PROMPT):\n",
    "    null_prompt = null_prompt\n",
    "    prompts = prompts\n",
    "\n",
    "    clip_text_embedder = CLIPTextEmbedder(device=get_device())\n",
    "    clip_text_embedder.load_submodels(**pt.embedder_submodels)\n",
    "\n",
    "    null_cond = clip_text_embedder(null_prompt)\n",
    "    torch.save(null_cond, join(EMBEDDED_PROMPTS_DIR, \"null_cond.pt\"))\n",
    "    print(\n",
    "        \"Null prompt embedding saved at: \",\n",
    "        f\"{join(EMBEDDED_PROMPTS_DIR, 'null_cond.pt')}\",\n",
    "    )\n",
    "\n",
    "    embedded_prompts = clip_text_embedder(prompts)\n",
    "    torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_prompts.pt\"))\n",
    "\n",
    "    print(\n",
    "        \"Prompts embeddings saved at: \",\n",
    "        f\"{join(EMBEDDED_PROMPTS_DIR, 'embedded_prompts.pt')}\",\n",
    "    )\n",
    "\n",
    "    get_memory_status()\n",
    "    clip_text_embedder.to(\"cpu\")\n",
    "    del clip_text_embedder\n",
    "    torch.cuda.empty_cache()\n",
    "    get_memory_status()\n",
    "\n",
    "    return embedded_prompts, null_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUnuGtHYdQIx"
   },
   "outputs": [],
   "source": [
    "embedded_prompts, null_prompt = embed_and_save_prompts(PROMPT)\n",
    "embedding = embedded_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GddsTwnzdQIx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ep_tensor = embedded_prompts.cpu().detach()\n",
    "ep = ep_tensor.numpy()\n",
    "\n",
    "plist = []\n",
    "for i in range(0,77):\n",
    "    plist.append(ep[0][i])\n",
    "    #print(ep[0][i])\n",
    "    #break\n",
    "\n",
    "print(\"Clip Embedding Vector:\")\n",
    "print(\"Norm of Difference of Successive Vectors\")\n",
    "print(\"\")\n",
    "\n",
    "for i in range(0,76):\n",
    "    tmp_v = plist[i+1] - plist[i]\n",
    "    #print(tmp_v)\n",
    "    l2 = numpy.linalg.norm(tmp_v, ord=2)\n",
    "    l1 = numpy.linalg.norm(tmp_v, ord=1)\n",
    "    std = numpy.std(tmp_v)\n",
    "\n",
    "    v1 = plist[i]\n",
    "    v2 = plist[i+1]\n",
    "    cos = np.dot(v1,v2)/(norm(v2)*norm(v2))\n",
    "    print(\"i=\", i, \" l1= \",l1, \" l2= \", l2, \" cos= \", cos, \" std= \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnoKJGmfdQIy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "\n",
    "ep_tensor = embedded_prompts.cpu().detach()\n",
    "ep = ep_tensor.numpy()\n",
    "\n",
    "plist = []\n",
    "for i in range(0,77):\n",
    "    plist.append(ep[0][i])\n",
    "    #print(ep[0][i])\n",
    "    #break\n",
    "\n",
    "print(\"Clip Embedding Vector:\")\n",
    "print(\"Size and Norm of Vectors\")\n",
    "print(\"\")\n",
    "\n",
    "for i in range(0,77):\n",
    "    v1 = plist[i]\n",
    "    l1 = numpy.linalg.norm(v1, ord=1)\n",
    "    l2 = numpy.linalg.norm(v1, ord=2)\n",
    "    std = numpy.std(v1)\n",
    "    std2 = std*std\n",
    "\n",
    "    print(\"i=\", i, \" l1= \",l1, \" l2= \", l2, \" std= \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGQNKILGdQI0"
   },
   "outputs": [],
   "source": [
    "# check the shape of the embedded prompts\n",
    "embedding_shape = tuple(embedded_prompts.shape)\n",
    "embedding_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzHUeJ5udQI0"
   },
   "outputs": [],
   "source": [
    "# check mean and std to use the same for the noise generation\n",
    "# one idea is to use one distribution per position (in the 77 positions)\n",
    "# in this case we would check the mean and std along dimension 2\n",
    "# embedded_prompts.mean(dim=2), embedded_prompts.std(dim=2)\n",
    "embedding_mean, embedding_std = embedded_prompts.mean(), embedded_prompts.std()\n",
    "embedding_mean, embedding_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7rteR6rPdQI1"
   },
   "outputs": [],
   "source": [
    "embedded_prompts.mean(dim=2), embedded_prompts.std(dim=2)\n",
    "noise = torch.normal(mean=embedded_prompts.mean(dim=2), std=embedded_prompts.std(dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZSiOGTUdQI1"
   },
   "outputs": [],
   "source": [
    "dist = torch.distributions.normal.Normal(\n",
    "    loc=embedded_prompts.mean(dim=2), scale=embedded_prompts.std(dim=2)\n",
    ")\n",
    "noise = dist.sample(sample_shape=torch.Size([768])).permute(1, 0, 2).permute(0, 2, 1)\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9kVIoOaxdQI1"
   },
   "outputs": [],
   "source": [
    "differences = []\n",
    "differences_means = []\n",
    "differences_stds = []\n",
    "dot_products = []\n",
    "for i, row in enumerate(embedded_prompts.squeeze()):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    diff = row - embedded_prompts.squeeze()[0]\n",
    "    differences.append(diff)\n",
    "    differences_means.append(diff.mean())\n",
    "    differences_stds.append(diff.std())\n",
    "    dot_products.append(torch.dot(row, embedded_prompts.squeeze()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "W_V65P0MdQI2"
   },
   "outputs": [],
   "source": [
    "differences = torch.stack(differences)\n",
    "differences_means = torch.stack(differences_means)\n",
    "differences_stds = torch.stack(differences_stds)\n",
    "dot_products = torch.stack(dot_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8B1zgfbjdQI2"
   },
   "outputs": [],
   "source": [
    "differences_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqiO-ahmdQI2"
   },
   "outputs": [],
   "source": [
    "differences_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1cA1Eu4dQI2"
   },
   "outputs": [],
   "source": [
    "dot_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVaI3i7TdQI2"
   },
   "outputs": [],
   "source": [
    "# generate noise and add to the embedded prompt\n",
    "generator = torch.Generator(device=DEVICE).manual_seed(SEED)\n",
    "# noise = torch.normal(\n",
    "#     mean=embedding_mean.item(),\n",
    "#     std=embedding_std.item(),\n",
    "#     size=embedding_shape,\n",
    "#     device=DEVICE,\n",
    "#     generator=generator,\n",
    "# )\n",
    "# noise.shape\n",
    "\n",
    "dist = torch.distributions.normal.Normal(\n",
    "    loc=embedded_prompts.mean(dim=2), scale=embedded_prompts.std(dim=2)\n",
    ")\n",
    "noise = dist.sample(sample_shape=torch.Size([768])).permute(1, 0, 2).permute(0, 2, 1)\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Q4GuWrdxdQI3"
   },
   "outputs": [],
   "source": [
    "embedding_e = embedded_prompts + 0.1 * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJJ6dTO2dQI3"
   },
   "outputs": [],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DdfDKIyYdQI3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "sd = StableDiffusion(device=DEVICE)\n",
    "sd.quick_initialize().load_autoencoder(**pt.autoencoder).load_decoder(**pt.decoder)\n",
    "sd.model.load_unet(**pt.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUrpMldldQI3"
   },
   "outputs": [],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjI7dj_gdQI4"
   },
   "outputs": [],
   "source": [
    "image = sd.generate_images_from_embeddings(\n",
    "    seed=SEED, embedded_prompt=embedded_prompts, null_prompt=null_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GjbH1gsdQI4"
   },
   "outputs": [],
   "source": [
    "to_pil(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mo8QSFAOdQI4"
   },
   "outputs": [],
   "source": [
    "image_e = sd.generate_images_from_embeddings(\n",
    "    seed=SEED, embedded_prompt=embedding_e, null_prompt=null_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws_80ZUQdQI5"
   },
   "outputs": [],
   "source": [
    "to_pil(image_e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_xIVDWKpdQI5"
   },
   "outputs": [],
   "source": [
    "def generate_images_from_disturbed_embeddings(\n",
    "    sd: StableDiffusion,\n",
    "    embedded_prompt: torch.Tensor,\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    noise_multiplier=NOISE_MULTIPLIER,\n",
    "    batch_size=BATCH_SIZE,\n",
    "):\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    embedding_mean, embedding_std = embedded_prompt.mean(), embedded_prompt.std()\n",
    "    embedding_shape = tuple(embedded_prompt.shape)\n",
    "\n",
    "    # noise = torch.normal(\n",
    "    #     mean=embedding_mean.item(),\n",
    "    #     std=embedding_std.item(),\n",
    "    #     size=embedding_shape,\n",
    "    #     device=device,\n",
    "    #     generator=generator,\n",
    "    # )\n",
    "    # test with standard normal distribution\n",
    "    # noise = torch.normal(\n",
    "    #     mean=0.0,\n",
    "    #     std=1.0,\n",
    "    #     size=embedding_shape,\n",
    "    #     device=device,\n",
    "    #     generator=generator,\n",
    "    # )\n",
    "    # embedded_prompt.mean(dim=2), embedded_prompt.std(dim=2)\n",
    "    # noise = torch.normal(\n",
    "    #     mean=embedded_prompt.mean(dim=2), std=embedded_prompt.std(dim=2)\n",
    "    # )\n",
    "    dist = torch.distributions.normal.Normal(\n",
    "        loc=embedded_prompt.mean(dim=2), scale=embedded_prompt.std(dim=2)\n",
    "    )\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        j = num_iterations - i\n",
    "\n",
    "        noise_i = (\n",
    "            dist.sample(sample_shape=torch.Size([768]))\n",
    "            .permute(1, 0, 2)\n",
    "            .permute(0, 2, 1)\n",
    "        )\n",
    "        noise_j = (\n",
    "            dist.sample(sample_shape=torch.Size([768]))\n",
    "            .permute(1, 0, 2)\n",
    "            .permute(0, 2, 1)\n",
    "        )\n",
    "        embedding_e = embedded_prompt + (\n",
    "            (i * noise_multiplier) * noise_i + (j * noise_multiplier) * noise_j\n",
    "        ) / (2 * num_iterations)\n",
    "\n",
    "        image_e = sd.generate_images_from_embeddings(\n",
    "            seed=seed,\n",
    "            embedded_prompt=embedding_e,\n",
    "            null_prompt=null_prompt,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        yield (image_e, embedding_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IctnyWRrdQI5"
   },
   "outputs": [],
   "source": [
    "image_generator = generate_images_from_disturbed_embeddings(\n",
    "    sd, embedded_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjAgD_99dQI5"
   },
   "outputs": [],
   "source": [
    "yielded = list(image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OZdgMCDfdQI5"
   },
   "outputs": [],
   "source": [
    "images = [image[0] for image in yielded]\n",
    "embeddings = [image[1] for image in yielded]\n",
    "hashs = [calculate_sha256(image[0]) for image in yielded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkNqKBZydQI6"
   },
   "outputs": [],
   "source": [
    "show_image_grid(torch.cat(images), nrow=int(math.log(NUM_ITERATIONS, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Ay3_VOsUdQI6"
   },
   "outputs": [],
   "source": [
    "pil_images = list(map(to_pil, map(torch.Tensor.squeeze, images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNA28DutdQI6"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import clip\n",
    "\n",
    "\n",
    "class AestheticPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "chadscorer_path = join(\"./input/model/aesthetic_scorer\", \"chadscorer.pth\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pt_state = torch.load(chadscorer_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# CLIP embedding dim is 768 for CLIP ViT L 14\n",
    "predictor = AestheticPredictor(768)\n",
    "predictor.load_state_dict(pt_state)\n",
    "predictor.to(device)\n",
    "predictor.eval()\n",
    "\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "\n",
    "def get_image_features(\n",
    "    image, device=device, model=clip_model, preprocess=clip_preprocess\n",
    "):\n",
    "    image = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        # l2 normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    image_features = image_features.cpu().detach().numpy()\n",
    "    return image_features\n",
    "\n",
    "\n",
    "def get_score(image):\n",
    "    image_features = get_image_features(image)\n",
    "    score = predictor(torch.from_numpy(image_features).to(device).float())\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7x67iaEdQI7"
   },
   "outputs": [],
   "source": [
    "images_data = []\n",
    "manifest = []\n",
    "\n",
    "for i, image in enumerate(pil_images):\n",
    "    # hash = hashlib.sha256(image).hexdigest()\n",
    "    image_tensor = images[i]\n",
    "    image_name = f\"image_{i}.png\"\n",
    "    image_path = os.path.abspath(join(IMAGES_DIR, image_name))\n",
    "    image_hash = calculate_sha256(image_tensor)\n",
    "    image_features = get_image_features(image)\n",
    "    image_score = get_score(image)\n",
    "    manifest_i = {\n",
    "        \"file-name\": image_name,\n",
    "        \"file-hash\": image_hash,\n",
    "        \"file-path\": image_path,\n",
    "        \"aesthetic-score\": image_score,\n",
    "        \"initial-prompt\": PROMPT,\n",
    "    }\n",
    "    manifest.append(manifest_i)\n",
    "\n",
    "    json_output_i = manifest_i.copy()\n",
    "    json_output_i[\"initial-prompt\"] = PROMPT\n",
    "    json_output_i[\"embedding-tensor\"] = embedding.tolist()\n",
    "    json_output_i[\"clip-vector\"] = image_features.tolist()\n",
    "    images_data.append(json_output_i)\n",
    "    image.save(image_path)\n",
    "    # images_data.append(\n",
    "    #     {\n",
    "    #         \"file-name\": image_name,\n",
    "    #         \"file-hash\": image_hash,\n",
    "    #         \"file-path\": image_path,\n",
    "    #         \"aesthetic-score\": image_score,\n",
    "    #         \"initial-prompt\": PROMPT,\n",
    "    #         \"embedding-tensor\": embeddings[i].tolist(),\n",
    "    #         \"clip-vector\": image_features.tolist(),\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "json.dump(images_data, open(join(FEATURES_DIR, \"features.json\"), \"w\"), indent=4)\n",
    "print(\"Features saved at: \", join(FEATURES_DIR, \"features.json\"))\n",
    "json.dump(manifest, open(join(OUTPUT_DIR, \"manifest.json\"), \"w\"), indent=4)\n",
    "print(\"Manifest saved at: \", join(OUTPUT_DIR, \"manifest.json\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
