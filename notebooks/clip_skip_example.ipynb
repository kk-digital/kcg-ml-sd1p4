{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZnyimbC2lrG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/kk-digital/kcg-ml-sd1p4\n",
        "%cd kcg-ml-sd1p4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "gK7Bb9Vr5BkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import zipfile\n",
        "import json\n",
        "from PIL import Image\n",
        "import requests\n",
        "import clip\n",
        "import numpy as np\n",
        "from transformers import CLIPModel, CLIPImageProcessor\n",
        "import torch\n",
        "\n",
        "sys.path.insert(0, os.getcwd())\n",
        "from utility.clip.clip_feature_zip_loader import ClipFeatureZipLoader\n",
        "from utility.clip.clip import ClipModel\n"
      ],
      "metadata": {
        "id": "DCQa2YRU2rbX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_clip_skip(layer_number=None):\n",
        "    clip_model = 'openai/clip-vit-large-patch14'\n",
        "    model = CLIPModel.from_pretrained(clip_model)\n",
        "\n",
        "    processor = CLIPImageProcessor.from_pretrained(clip_model)\n",
        "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    vision_outputs = model.vision_model(\n",
        "        **inputs,\n",
        "        output_hidden_states=True,\n",
        "    )\n",
        "\n",
        "    # Use the layer before the last one if no layer_number is provided\n",
        "    if layer_number is None:\n",
        "        layer_number = len(vision_outputs.hidden_states) - 2\n",
        "\n",
        "    layer_output = vision_outputs.hidden_states[layer_number]\n",
        "    pooled_output = layer_output[:, 0, :]\n",
        "    pooled_output = model.vision_model.post_layernorm(pooled_output)\n",
        "    image_features = model.visual_projection(pooled_output)\n",
        "    print(image_features.shape)\n",
        "    assert image_features.shape == torch.Size([1, 768])\n",
        "\n",
        "# Use the function without specifying a layer; it will use the layer before the last\n",
        "test_clip_skip()\n",
        "\n",
        "# Or specify a layer when calling the function\n",
        "#layer_number = 4\n",
        "#test_clip_skip(layer_number)\n"
      ],
      "metadata": {
        "id": "30bt24pv2rdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_clip_model_clip_skip():\n",
        "    clip_model = ClipModel(verbose=True, clip_skip=True)\n",
        "    clip_model.load_clip()\n",
        "\n",
        "    # sample image\n",
        "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "    image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "    image_features, penultimate_output = clip_model.get_image_features(image)\n",
        "    print(image_features.shape)\n",
        "    clip_model.unload_clip()\n",
        "\n",
        "    assert image_features.shape == torch.Size([1, 768])\n",
        "\n",
        "# Execute the function\n",
        "test_clip_model_clip_skip()\n"
      ],
      "metadata": {
        "id": "j23xSwdx34J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHMwN0fI644_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}