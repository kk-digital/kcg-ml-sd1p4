{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kk-digital/kcg-ml-sd1p4\n",
    "%cd kcg-ml-sd1p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./download_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ./process_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "from os.path import join\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "output_base_dir = join(base_directory, \"./output/sd2-notebook/\")\n",
    "output_directory = join(output_base_dir, \"distributions/\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(output_directory)\n",
    "except Exception as e:\n",
    "    print(e, \"\\n\", \"Creating the path...\")\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "from stable_diffusion.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.utils_backend import *\n",
    "from stable_diffusion.utils_image import *\n",
    "from stable_diffusion.utils_model import *\n",
    "from configs.model_config import ModelPathConfig\n",
    "from stable_diffusion import CLIPconfigs, SDconfigs\n",
    "\n",
    "config = ModelPathConfig()\n",
    "\n",
    "\n",
    "to_pil = lambda image: ToPILImage()(torch.clamp((image + 1.0) / 2.0, min=0.0, max=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the logistic distribution\n",
    "def logistic_distribution(loc, scale):\n",
    "    base_distribution = torch.distributions.Uniform(0, 1)\n",
    "    transforms = [\n",
    "        torch.distributions.transforms.SigmoidTransform().inv,\n",
    "        torch.distributions.transforms.AffineTransform(loc=loc, scale=scale),\n",
    "    ]\n",
    "    logistic = torch.distributions.TransformedDistribution(\n",
    "        base_distribution, transforms\n",
    "    )\n",
    "    return logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty stable diffusion class\n",
    "stable_diffusion = StableDiffusion(device=device)\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a latent diffusion model so you can load its submodels from disk\n",
    "stable_diffusion.quick_initialize().load_submodel_tree(\n",
    "    **config.get_model(\n",
    "            SDconfigs.ENCODER,\n",
    "            SDconfigs.DECODER,\n",
    "            SDconfigs.AUTOENCODER,\n",
    "            CLIPconfigs.TEXT_EMBEDDER,\n",
    "            CLIPconfigs.TOKENIZER,\n",
    "            CLIPconfigs.TEXT_MODEL,\n",
    "            SDconfigs.UNET,\n",
    "            to_dict=True,\n",
    "        )\n",
    ")\n",
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the sampling is impacted by the variance of the distribution used\n",
    "\n",
    "temperature = 1.0\n",
    "\n",
    "var_0 = 0.4\n",
    "var_t = 0.6\n",
    "var_steps = 9\n",
    "\n",
    "var_range = torch.linspace(var_0, var_t, var_steps)\n",
    "\n",
    "images = []\n",
    "\n",
    "for var in var_range:\n",
    "    noise_fn = (\n",
    "        lambda shape, device=device: logistic_distribution(loc=0.0, scale=var.item())\n",
    "        .sample(shape)\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    imgs = stable_diffusion.generate_images(\n",
    "        prompt=\"A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta\",\n",
    "        seed=2982,\n",
    "        noise_fn=noise_fn,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    images.append(imgs)\n",
    "\n",
    "images = torch.cat(images, dim=0)\n",
    "grid = torchvision.utils.make_grid(\n",
    "    images, normalize=False, nrow=3, range=(-1, 1), scale_each=True, pad_value=0\n",
    ")\n",
    "grid_img = to_pil(grid)\n",
    "grid_img.save(\n",
    "    join(\n",
    "        output_directory,\n",
    "        f\"test_dist_temp{temperature:.3f}_eta{stable_diffusion.ddim_eta:.3f}_logistic_var{var_0:.3f}{var_t:.3f}.png\",\n",
    "    )\n",
    ")\n",
    "grid_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
